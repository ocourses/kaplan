[
  {
    "objectID": "src/derivees/equations.html",
    "href": "src/derivees/equations.html",
    "title": "Équations Variationnelles",
    "section": "",
    "text": "Les équations variationnelles offrent une méthode directe pour calculer les dérivées des solutions des équations différentielles par rapport aux conditions initiales et aux paramètres. Elles permettent de linéariser le problème autour de la solution nominale.",
    "crumbs": [
      "Plan du cours",
      "Dérivées",
      "Equations variationnelles"
    ]
  },
  {
    "objectID": "src/derivees/equations.html#problème-de-cauchy-pour-les-équations-variationnelles",
    "href": "src/derivees/equations.html#problème-de-cauchy-pour-les-équations-variationnelles",
    "title": "Équations Variationnelles",
    "section": "Problème de Cauchy pour les Équations Variationnelles",
    "text": "Problème de Cauchy pour les Équations Variationnelles\nConsidérons une EDO dépendant de paramètres \\theta :\n\n\\dot{x}(t) = f(t, x(t), \\theta), \\quad x(t_0) = x_0\n\nPour trouver la dérivée de la solution x(t, x_0, \\theta) par rapport à x_0 ou \\theta, nous devons résoudre un problème de Cauchy pour les équations variationnelles.\n\n1. Dérivée par Rapport à x_0 :\nLa dérivée X(t) = \\frac{\\partial x}{\\partial x_0}(t, x_0, \\theta) satisfait l’équation variationnelle suivante :\n\n\\frac{d}{dt} X(t) = \\frac{\\partial f}{\\partial x}(t, x(t, x_0, \\theta), \\theta) \\, X(t)\n\nAvec la condition initiale :\n\nX(t_0) = I\n\n\n\n2. Dérivée par Rapport à \\theta :\nLa dérivée Y(t) = \\frac{\\partial x}{\\partial \\theta}(t, x_0, \\theta) satisfait l’équation variationnelle suivante :\n\n\\frac{d}{dt} Y(t) = \\frac{\\partial f}{\\partial x}(t, x(t, x_0, \\theta), \\theta) \\, Y(t) + \\frac{\\partial f}{\\partial \\theta}(t, x(t, x_0, \\theta), \\theta)\n\nAvec la condition initiale :\n\nY(t_0) = 0",
    "crumbs": [
      "Plan du cours",
      "Dérivées",
      "Equations variationnelles"
    ]
  },
  {
    "objectID": "src/derivees/equations.html#résolution-des-équations-variationnelles",
    "href": "src/derivees/equations.html#résolution-des-équations-variationnelles",
    "title": "Équations Variationnelles",
    "section": "Résolution des Équations Variationnelles",
    "text": "Résolution des Équations Variationnelles\nLes équations variationnelles peuvent être résolues en utilisant soit l’exponentielle de matrice (dans le cas linéaire), soit des schémas numériques comme les méthodes de Runge-Kutta.\n\n1. Exponentielle de Matrice :\nDans le cas linéaire \\dot{x}(t) = A x(t), la solution de l’équation variationnelle pour X(t) = \\frac{\\partial x}{\\partial x_0}(t, x_0) est donnée par :\n\nX(t) = e^{tA}\n\n\n\n2. Schémas de Runge-Kutta :\nPour des systèmes plus complexes ou lorsque A dépend de paramètres, les méthodes de Runge-Kutta peuvent être utilisées pour résoudre numériquement les équations variationnelles.\nPar exemple, pour l’EDO \\dot{x}(t) = A x(t) + b(t), les schémas de Runge-Kutta peuvent être appliqués directement aux équations variationnelles.",
    "crumbs": [
      "Plan du cours",
      "Dérivées",
      "Equations variationnelles"
    ]
  },
  {
    "objectID": "src/integration_numerique/runge_kutta.html",
    "href": "src/integration_numerique/runge_kutta.html",
    "title": "Schémas de Runge-Kutta",
    "section": "",
    "text": "Exercice :\nConsidérons le schéma de Runge-Kutta suivant :\n\n\\begin{aligned}\nk_1 &= f(t_0, x_0) \\\\\nk_2 &= f(t_0 + c_2h, x_0 + a_{21}hk_1) \\\\\nx_1 &= x_0 + h(b_1k_1 + b_2k_2)\n\\end{aligned}\n\n\nMontrer que (c_2 = a_{21}) :\n\nEn utilisant le développement de Taylor pour (f) autour de ((t_0, x_0)), nous avons :\n\nf(t_0 + c_2h, x_0 + a_{21}hk_1) = f(t_0, x_0) + c_2h \\frac{\\partial f}{\\partial t} + a_{21}hk_1 \\frac{\\partial f}{\\partial x} + O(h^2)\n\nPour que le schéma soit d’ordre 2, les termes d’ordre 1 doivent correspondre, ce qui implique (c_2 = a_{21}).\n\nRelations pour l’Ordre 2 :\n\nPour que le schéma soit d’ordre 2, les coefficients doivent satisfaire les conditions suivantes :\n\nb_1 + b_2 = 1\n\n\nb_2 c_2 = \\frac{1}{2}\n\nEn substituant (c_2 = a_{21}) dans la deuxième équation, nous obtenons :\n\nb_2 a_{21} = \\frac{1}{2}\n\n\nConclusion :\n\nEn résolvant ces équations, nous trouvons que (b_1 = 0), (b_2 = 1), et (a_{21} = ).\nCes valeurs correspondent à la méthode de Runge-Kutta d’ordre 2, également connue sous le nom de méthode du point milieu.",
    "crumbs": [
      "Plan du cours",
      "Intégration numérique",
      "Méthode de Runge-Kutta"
    ]
  },
  {
    "objectID": "src/integration_numerique/runge_kutta.html#tableau-de-butcher",
    "href": "src/integration_numerique/runge_kutta.html#tableau-de-butcher",
    "title": "Schémas de Runge-Kutta",
    "section": "Tableau de Butcher",
    "text": "Tableau de Butcher\nUn schéma de Runge-Kutta est défini par le tableau suivant :\n\n\\begin{array}{c|cccc}\n0 &  &  &  &  \\\\\nc_2 & a_{21} &  &  &  \\\\\n\\vdots & \\vdots & \\ddots &  &  \\\\\nc_s & a_{s1} & a_{s2} & \\cdots & a_{s,s-1} \\\\\n\\hline\n& b_1 & b_2 & \\cdots & b_s \\\\\n\\end{array}",
    "crumbs": [
      "Plan du cours",
      "Intégration numérique",
      "Méthode de Runge-Kutta"
    ]
  },
  {
    "objectID": "src/integration_numerique/runge_kutta.html#exemples-de-schémas-de-runge-kutta",
    "href": "src/integration_numerique/runge_kutta.html#exemples-de-schémas-de-runge-kutta",
    "title": "Schémas de Runge-Kutta",
    "section": "Exemples de Schémas de Runge-Kutta",
    "text": "Exemples de Schémas de Runge-Kutta\n\nMéthode d’Euler :\n\nTableau de Butcher :\n\n\\begin{array}{c|c}\n0 & 0 \\\\\n\\hline\n& 1 \\\\\n\\end{array}\n\nOrdre : 1\n\nMéthode de Runge-Kutta d’Ordre 2 (Point Milieu) :\n\nTableau de Butcher :\n\n\\begin{array}{c|cc}\n0 & 0 & 0 \\\\\n\\frac{1}{2} & \\frac{1}{2} & 0 \\\\\n\\hline\n& 0 & 1 \\\\\n\\end{array}\n\nOrdre : 2\n\nMéthode de Heun :\n\nTableau de Butcher :\n\n\\begin{array}{c|cc}\n0 & 0 & 0 \\\\\n1 & 1 & 0 \\\\\n\\hline\n& \\frac{1}{2} & \\frac{1}{2} \\\\\n\\end{array}\n\nOrdre : 2\n\nMéthode de Runge-Kutta d’Ordre 4 (RK4) :\n\nTableau de Butcher :\n\n\\begin{array}{c|cccc}\n0 & 0 & 0 & 0 & 0 \\\\\n\\frac{1}{2} & \\frac{1}{2} & 0 & 0 & 0 \\\\\n\\frac{1}{2} & 0 & \\frac{1}{2} & 0 & 0 \\\\\n1 & 0 & 0 & 1 & 0 \\\\\n\\hline\n& \\frac{1}{6} & \\frac{1}{3} & \\frac{1}{3} & \\frac{1}{6} \\\\\n\\end{array}\n\nOrdre : 4",
    "crumbs": [
      "Plan du cours",
      "Intégration numérique",
      "Méthode de Runge-Kutta"
    ]
  },
  {
    "objectID": "src/edo_lineaires/exemples.html",
    "href": "src/edo_lineaires/exemples.html",
    "title": "Exemples et Exercices",
    "section": "",
    "text": "Considérons le système linéaire suivant :\n\n   x'(t) = \\begin{pmatrix} -1 & 0 \\\\ 0 & -2 \\end{pmatrix} x(t), \\quad x(0) = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\n\n\nCalcul de l’Exponentielle de la Matrice :\n\nLa matrice A = \\begin{pmatrix} -1 & 0 \\\\ 0 & -2 \\end{pmatrix} est diagonale.\nL’exponentielle de A est : \n  e^{tA} = \\begin{pmatrix} e^{-t} & 0 \\\\ 0 & e^{-2t} \\end{pmatrix}\n\n\nSolution du Système :\n\nLa solution est donnée par : \n  x(t) = e^{tA} x(0) = \\begin{pmatrix} e^{-t} & 0 \\\\ 0 & e^{-2t} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} e^{-t} \\\\ e^{-2t} \\end{pmatrix}\n\n\nComportement à l’Infini :\n\nLorsque t \\to \\infty, x(t) \\to \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} car les valeurs propres -1 et -2 sont négatives.",
    "crumbs": [
      "Plan du cours",
      "Problème de Cauchy linéaire",
      "Exemples"
    ]
  },
  {
    "objectID": "src/edo_lineaires/exemples.html#exemple-1-convergence",
    "href": "src/edo_lineaires/exemples.html#exemple-1-convergence",
    "title": "Exemples et Exercices",
    "section": "",
    "text": "Considérons le système linéaire suivant :\n\n   x'(t) = \\begin{pmatrix} -1 & 0 \\\\ 0 & -2 \\end{pmatrix} x(t), \\quad x(0) = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\n\n\nCalcul de l’Exponentielle de la Matrice :\n\nLa matrice A = \\begin{pmatrix} -1 & 0 \\\\ 0 & -2 \\end{pmatrix} est diagonale.\nL’exponentielle de A est : \n  e^{tA} = \\begin{pmatrix} e^{-t} & 0 \\\\ 0 & e^{-2t} \\end{pmatrix}\n\n\nSolution du Système :\n\nLa solution est donnée par : \n  x(t) = e^{tA} x(0) = \\begin{pmatrix} e^{-t} & 0 \\\\ 0 & e^{-2t} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} e^{-t} \\\\ e^{-2t} \\end{pmatrix}\n\n\nComportement à l’Infini :\n\nLorsque t \\to \\infty, x(t) \\to \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} car les valeurs propres -1 et -2 sont négatives.",
    "crumbs": [
      "Plan du cours",
      "Problème de Cauchy linéaire",
      "Exemples"
    ]
  },
  {
    "objectID": "src/edo_lineaires/exemples.html#exemple-2-divergence",
    "href": "src/edo_lineaires/exemples.html#exemple-2-divergence",
    "title": "Exemples et Exercices",
    "section": "Exemple 2 : Divergence",
    "text": "Exemple 2 : Divergence\nConsidérons le système linéaire suivant :\n\n    x'(t) = \\begin{pmatrix} 1 & -1 \\\\ 1 & 1 \\end{pmatrix} x(t), \\quad x(0) = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}\n\n\nCalcul de l’Exponentielle de la Matrice :\n\nLa matrice A = \\begin{pmatrix} 1 & -1 \\\\ 1 & 1 \\end{pmatrix} a des valeurs propres complexes conjuguées 1 \\pm i.\nL’exponentielle de A est : \n  e^{tA} = e^t \\begin{pmatrix} \\cos(t) & -\\sin(t) \\\\ \\sin(t) & \\cos(t) \\end{pmatrix}\n\n\nSolution du Système :\n\nLa solution est donnée par : \n  x(t) = e^{tA} x(0) = e^t \\begin{pmatrix} \\cos(t) & -\\sin(t) \\\\ \\sin(t) & \\cos(t) \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = e^t \\begin{pmatrix} \\cos(t) \\\\ \\sin(t) \\end{pmatrix}\n\n\nComportement à l’Infini :\n\nLorsque t \\to \\infty, \\|x(t)\\| \\to \\infty car la partie réelle des valeurs propres est positive.",
    "crumbs": [
      "Plan du cours",
      "Problème de Cauchy linéaire",
      "Exemples"
    ]
  },
  {
    "objectID": "src/edo_lineaires/exemples.html#exemple-3-second-membre-constant",
    "href": "src/edo_lineaires/exemples.html#exemple-3-second-membre-constant",
    "title": "Exemples et Exercices",
    "section": "Exemple 3 : Second Membre Constant",
    "text": "Exemple 3 : Second Membre Constant\nConsidérons le système linéaire suivant avec un second membre constant :\n\n    x'(t) = \\begin{pmatrix} -1 & 0 \\\\ 0 & -2 \\end{pmatrix} x(t) + \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}, \\quad x(0) = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\n\n\nSolution du Système :\n\nLa solution est donnée par : \n  x(t) = e^{tA} x(0) + \\int_{0}^{t} e^{(t-s)A} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} \\mathrm{d}s\n\nCalculons l’intégrale : \n   x(t) = \\begin{pmatrix} e^{-t} & 0 \\\\ 0 & e^{-2t} \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} + \\int_{0}^{t} \\begin{pmatrix} e^{-(t-s)} & 0 \\\\ 0 & e^{-2(t-s)} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} \\, \\mathrm{d}s\n\nCe qui donne : \n   x(t) = \\begin{pmatrix} 1 - e^{-t} \\\\ \\frac{1}{2} (1 - e^{-2t}) \\end{pmatrix}\n\n\nComportement à l’Infini :\n\nLorsque t \\to \\infty, x(t) \\to \\begin{pmatrix} 1 \\\\ \\frac{1}{2} \\end{pmatrix}.",
    "crumbs": [
      "Plan du cours",
      "Problème de Cauchy linéaire",
      "Exemples"
    ]
  },
  {
    "objectID": "src/edo_lineaires/exponentielle.html",
    "href": "src/edo_lineaires/exponentielle.html",
    "title": "Exponentielle de Matrice",
    "section": "",
    "text": "L’exponentielle d’une matrice est un outil fondamental pour résoudre les équations différentielles linéaires. Elle généralise la notion d’exponentielle des nombres réels aux matrices.",
    "crumbs": [
      "Plan du cours",
      "Problème de Cauchy linéaire",
      "Exponentielle de matrice"
    ]
  },
  {
    "objectID": "src/edo_lineaires/exponentielle.html#définition",
    "href": "src/edo_lineaires/exponentielle.html#définition",
    "title": "Exponentielle de Matrice",
    "section": "Définition",
    "text": "Définition\nOn définit l’exponentielle d’une matrice A \\in \\mathcal{M}_n(\\mathbb{R}) par :\n\ne^A \\coloneqq \\sum_{k=0}^{\\infty} \\frac{A^k}{k!} \\in \\mathcal{M}_n(\\mathbb{R}).\n\n\n\n\n\n\n\nNote\n\n\n\nL’exponentielle de A s’écrit aussi \\exp(A).\n\n\n\n\n\n\n\n\nPreuve de la convergence de la série\n\n\n\n\n\nPour montrer que cette série converge absolument pour toute matrice A \\in \\mathcal{M}_n(\\mathbb{R}), nous allons utiliser une norme sous-multiplicative. Soit \\|\\cdot\\| une norme sous-multiplicative sur \\mathcal{M}_n(\\mathbb{R}), par exemple la norme de Frobenius ou n’importe quelle norme subordonnée (ou norme d’algèbre). Une norme sous-multiplicative satisfait pour toutes matrices \\ A, B \\in \\mathcal{M}_n(\\mathbb{R}) la propriété suivante :\n\n    \\|A B\\| \\leq \\|A\\| \\|B\\|.\n\nDans ce cas, nous pouvons majorer la norme de chaque terme de la série :\n\n    \\left\\| \\frac{A^k}{k!} \\right\\| = \\frac{\\|A^k\\|}{k!} \\leq \\frac{\\|A\\|^k}{k!}.\n\nLa série \\sum_{k=0}^{\\infty} \\frac{\\|A\\|^k}{k!} est simplement l’exponentielle scalaire de la norme \\|A\\| :\n\n    \\sum_{k=0}^{\\infty} \\frac{\\|A\\|^k}{k!} = e^{\\|A\\|}.\n\nPuisque la série de droite converge, il en découle que la série de matrices \\sum_{k=0}^{\\infty} \\frac{A^k}{k!} converge absolument. Enfin, une série absolument convergente dans un espace vectoriel normé de dimension finie est convergente.",
    "crumbs": [
      "Plan du cours",
      "Problème de Cauchy linéaire",
      "Exponentielle de matrice"
    ]
  },
  {
    "objectID": "src/edo_lineaires/exponentielle.html#propriétés",
    "href": "src/edo_lineaires/exponentielle.html#propriétés",
    "title": "Exponentielle de Matrice",
    "section": "Propriétés",
    "text": "Propriétés\n\nMatrice Nulle\nOn note 0_n la matrice nulle de \\mathcal{M}_n(\\mathbb{R}) et I_n la matrice identité. On a alors : \n    e^{0_n} = I_n.\n\n\n\nMatrice Diagonale\nSi \\Sigma est une matrice diagonale avec des éléments diagonaux \\lambda_1, \\lambda_2, \\dots, \\lambda_n, c’est-à-dire :\n\n    \\Sigma = \\text{diag}(\\lambda_1, \\lambda_2, \\dots, \\lambda_n) =\n    \\begin{pmatrix}\n        \\lambda_1 & 0 & \\cdots & 0 \\\\\n        0 & \\lambda_2 & \\cdots & 0 \\\\\n        \\vdots & \\vdots & \\ddots & \\vdots \\\\\n        0 & 0 & \\cdots & \\lambda_n\n    \\end{pmatrix},\n\nalors l’exponentielle de \\Sigma est simplement la matrice diagonale des exponentielles des éléments diagonaux :\n\n    e^\\Sigma = \\text{diag}(e^{\\lambda_1}, e^{\\lambda_2}, \\dots, e^{\\lambda_n}) =\n    \\begin{pmatrix}\n        e^{\\lambda_1} & 0 & \\cdots & 0 \\\\\n        0 & e^{\\lambda_2} & \\cdots & 0 \\\\\n        \\vdots & \\vdots & \\ddots & \\vdots \\\\\n        0 & 0 & \\cdots & e^{\\lambda_n}\n    \\end{pmatrix}.\n\n\n\nMatrice Semblable\nSoit A \\in \\mathcal{M}_n(\\mathbb{R}) et P \\in \\mathrm{GL}_n(\\mathbb{R}), c’est-à-dire P est inversible. Alors\n\n    e^{PAP^{-1}} = P e^A P^{-1}.\n\n\n\n\n\n\n\nNote\n\n\n\nSi la matrice A est diagonalisable, on peut facilement calculer son exponentielle en se ramenant au calcul de l’exponentielle d’une matrice diagonale.\n\n\n\n\n\n\n\n\nPreuve\n\n\n\n\n\nNous avons : \n    e^{PAP^{-1}} = \\sum_{k=0}^{\\infty} \\frac{\\left(PAP^{-1}\\right)^k}{k!}.\n\nMontrons par récurrence que \\left(PAP^{-1}\\right)^k = P A^k P^{-1}.\n\nPour k=0, nous avons I_n = \\left(PAP^{-1}\\right)^0 et P A^0 P^{-1} = P P^{-1} = I_n donc la propriété est vraie.\nSupposons cette propriété vraie pour k &gt; 0. Alors \n  \\left(PAP^{-1}\\right)^{k+1} = \\left(PAP^{-1}\\right)^k PAP^{-1} = P A^k P^{-1} PAP^{-1} = P A^{k+1} P^{-1}\n et donc la propriété est vraie pour k+1.\n\nAinsi, on a : \\begin{align}\n    e^{PAP^{-1}} &= \\sum_{k=0}^{\\infty} \\frac{\\left(PAP^{-1}\\right)^k}{k!} =\n    \\sum_{k=0}^{\\infty} \\left( P \\frac{A^k}{k!} P^{-1} \\right) \\notag \\\\[1.5em]\n    &= P \\left( \\sum_{k=0}^{\\infty} \\frac{A^k}{k!} \\right) P^{-1}\n    \\tag*{(par continuité de la conjugaison)} \\\\[1.5em]\n    &= P e^A P^{-1}. \\notag\n\\end{align}\n\n\n\n\n\nMatrice Transposée\nLa transposée d’une matrice A de taille m \\times n est notée A^T et s’obtient en échangeant les lignes et les colonnes de A. Autrement dit, l’élément (i, j) de A^T est l’élément (j, i) de A :\n\n(A^T)_{ij} = A_{ji}\n\nConcernant l’exponentielle de matrice, nous avons :\n\ne^{A^T} = (e^A)^T.\n\n\n\n\n\n\n\nNote\n\n\n\nLa preuve est laissée en exercice. On pourra s’inspirée de la preuve ci-avant.\n\n\n\n\nMatrice Nilpotente\nUne matrice carrée A est dite nilpotente s’il existe un entier k tel que :\n\nA^k = 0.\n\nDans ce cas, l’exponentielle de A s’écrit sous forme de somme finie :\n\ne^A = \\sum_{n=0}^{k-1} \\frac{A^n}{n!}.\n\nPar exemple, pour la matrice\n\nA = \\begin{pmatrix} 0 & 1 \\\\ 0 & 0 \\end{pmatrix},\n\non a A^2 = 0, donc son exponentielle est :\n\ne^A = I + A = \\begin{pmatrix} 1 & 1 \\\\ 0 & 1 \\end{pmatrix}.\n\n\n\nMatrices Commutantes\nSi les matrices A et B commutent, c’est-à-dire si AB = BA, alors l’exponentielle de leur somme est le produit des exponentielles :\n\n    e^{A + B} = e^A\\, e^B.\n\n\n\n\n\n\n\nPreuve\n\n\n\n\n\nSoit A et B deux matrices dans \\mathcal{M}_n(\\mathbb{R}) qui commutent, c’est-à-dire telle que AB = BA.\nEn développant (A+B)^k à l’aide du binôme de Newton, on obtient puisque A et B commutent : \n    (A + B)^k = \\sum_{j=0}^{k} \\binom{k}{j} A^j B^{k-j}.\n\nEn utilisant le produit de Cauchy et le résultat précédent, nous avons : \n    e^A\\, e^B = \\sum_{k=0}^{\\infty} c_k, \\quad c_k = \\sum_{j=0}^{k} \\frac{A^j}{j!} \\frac{B^{k-j}}{(k-j)!} = \\frac{(A + B)^k}{k!}.\n\nAinsi, au final nous obtenons : \n    e^A\\, e^B = \\sum_{k=0}^{\\infty} \\frac{(A + B)^k}{k!} = e^{A+B}.\n\nCela conclut la démonstration.\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nLa commutativité des matrices A et B est une condition nécessaire pour que cette propriété soit vraie. Voir la formule de Baker-Campbell-Hausdorff pour plus de détails.\nContre-exemple. Soient \n    A = \\begin{pmatrix} 0 & 1 \\\\ 0 & 0 \\end{pmatrix}, \\quad B = \\begin{pmatrix} 0 & 0 \\\\ 1 & 0 \\end{pmatrix}.\n\nAlors, pusique A^2 = B^2 = 0_2, on a : \n    e^A = \\begin{pmatrix} 1 & 1 \\\\ 1 & 0 \\end{pmatrix}, \\quad e^B = \\begin{pmatrix} 1 & 0 \\\\ 1 & 1 \\end{pmatrix}\n mais par ailleursn (A + B)^2 = I_2 donc \n    \\begin{aligned}\n        e^{A+B} &= \\left( \\sum_{k=0}^{\\infty} \\frac{1}{2k!} \\right) I_2 + \\left( \\sum_{k=0}^{\\infty} \\frac{1}{(2k+1)!} \\right) (A+B) =\n        \\cosh(1) I_2 + \\sinh(1) (A + B) \\\\\n        &\\ne e^A e^B = \\begin{pmatrix} 2 & 1 \\\\ 1 & 1 \\end{pmatrix}.\n    \\end{aligned}\n\n\n\n\n\nInversibilité et Dérivabilité\nSoit A \\in \\mathcal{M}_n(\\mathbb{R}). En corollaire de ce qui précède, nous avons les propriétés suivantes :\n\nInversibilité : e^A est toujours une matrice inversible et son inverse est donné par :\n\n     (e^A)^{-1} = e^{-A}.\n\nDérivabilité : L’application t \\mapsto e^{tA} est dérivable par rapport à t, et sa dérivée est donnée par :\n\n     \\frac{\\mathrm{d}}{\\mathrm{d} t} e^{tA} = A\\, e^{tA} = e^{tA} A.\n\n\n\n\n\n\n\n\nPreuve\n\n\n\n\n\n\nInversibilité :\nPour prouver que e^A est inversible et que son inverse est e^{-A}, nous utilisons la commutativité des matrices A et -A. En effet, il est évident que les matrices A et -A commutent, donc on peut écrire :\n\n     e^{-A} e^A = e^A e^{-A} = e^{A + (-A)} = e^{0_n} = I_n.\n\nAinsi, e^A est inversible et son inverse est e^{-A}.\nDérivabilité :\nFixons un t \\in \\mathbb{R}. Pour étudier la dérivée de e^{tA} par rapport à t, nous commençons par utiliser la définition de la dérivée : \n     \\frac{\\mathrm{d}}{\\mathrm{d}t} e^{tA} = \\lim_{h \\to 0} \\frac{e^{(t+h)A} - e^{tA}}{h}.\n\nEn remarquant que tA et hA commutent, nous réécrivons la différence comme suit : \n     e^{(t+h)A} - e^{tA} = e^{tA} e^{hA} - e^{tA} = e^{tA} (e^{hA} - I_n).\n\nDivisons par h : \n     \\frac{e^{(t+h)A} - e^{tA}}{h} = e^{tA} \\frac{e^{hA} - I}{h} = e^{tA} \\left( \\sum_{k=1}^{\\infty} \\frac{h^{k-1}A^k}{k!} \\right).\n\nOr, \n     \\begin{aligned}\n         \\sum_{k=1}^{\\infty} \\frac{h^{k-1}A^k}{k!} &= A + h \\sum_{k=2}^{\\infty} \\frac{h^{k-2}A^k}{k!} \\\\\n         &= A + h A^2 \\sum_{k=0}^{\\infty} \\frac{h^{k}A^k}{(k+2)!}\n     \\end{aligned}\n et la série de termes {h^{k}A^k}/{(k+2)!} est convergente puisque ceux-ci sont majorés pas ceux de l’exponentielle de hA. On note B la limite de la série et on obtient \n     \\lim_{h \\to 0} \\frac{e^{(t+h)A} - e^{tA}}{h} = \\lim_{h \\to 0} e^{tA} \\left( A + h A^2 B \\right) = e^{tA} A.\n\nDonc, la dérivée de e^{tA} est :\n\n     \\frac{\\mathrm{d}}{\\mathrm{d}t} e^{tA} = e^{tA} A.\n\nPar commutativité de A avec ses puissances, on peut aussi écrire cette dérivée comme :\n\n     \\frac{\\mathrm{d}}{\\mathrm{d}t} e^{tA} = A\\, e^{tA}.\n\n\nCela conclut la démonstration des deux propriétés.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nOn pourra préférer la notation \\exp. Ces deux derniers résultats nous indique que l’on a \\exp \\colon \\mathcal{M}_n(\\mathbb{R}) \\to \\mathrm{GL}_n(\\mathbb{R}) et que \n    \\frac{\\mathrm{d}}{\\mathrm{d}t} \\exp(tA) = A \\exp(tA) = \\exp(tA) A.\n Attention, nous n’avons rien prouvé quant à la régularité de l’application \\exp, rien au sujet ni de sa continuité sur \\mathcal{M}_n(\\mathbb{R}), ni de sa dérivabilité. Cependant, on peut montrer que cette application est continue, même \\mathscr{C}^\\infty. L’application exponentielle est même surjective.",
    "crumbs": [
      "Plan du cours",
      "Problème de Cauchy linéaire",
      "Exponentielle de matrice"
    ]
  },
  {
    "objectID": "src/index.html",
    "href": "src/index.html",
    "title": "Cours Prépa INP",
    "section": "",
    "text": "Introduction\n\n\nÉquations différentielles ordinaires\nProblème de Kaplan\nProblème aux moindres carrés\n\n\n\n\n\n\nProblème de Cauchy linéaire\n\n\nDéfinition\nExponentielle de matrice\nSolution\nExemples\n\n\n\n\n\n\nIntégration numérique\n\n\nEuler\nMéthodes de Runge-Kutta\n\n\n\n\n\n\nDérivées\n\n\nFonctions dérivées\nEquations variationnelles\n\n\n\n\n\nNo matching items\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Plan du cours"
    ]
  },
  {
    "objectID": "src/introduction/kaplan.html",
    "href": "src/introduction/kaplan.html",
    "title": "Problème de Kaplan",
    "section": "",
    "text": "Le problème de Kaplan est un exemple concret d’estimation de paramètres et de la condition initiale dans le cadre d’une équation différentielle ordinaire linéaire vectorielle. Ce problème est souvent utilisé pour illustrer les techniques d’identification de systèmes dynamiques.",
    "crumbs": [
      "Plan du cours",
      "Introduction",
      "Problème de Kaplan"
    ]
  },
  {
    "objectID": "src/introduction/kaplan.html#présentation-du-problème",
    "href": "src/introduction/kaplan.html#présentation-du-problème",
    "title": "Problème de Kaplan",
    "section": "Présentation du Problème",
    "text": "Présentation du Problème\nConsidérons un système dynamique linéaire décrit par l’équation différentielle suivante :\n\nx'(t) = Ax(t) + Bu(t), \\quad x(0) = x_0\n\noù :\n\nx(t) est le vecteur d’état du système à l’instant t.\nA est la matrice des coefficients, représentant la dynamique interne du système.\nB est une matrice qui modélise l’influence des entrées u(t) sur le système.\nu(t) est le vecteur des entrées ou des commandes appliquées au système.",
    "crumbs": [
      "Plan du cours",
      "Introduction",
      "Problème de Kaplan"
    ]
  },
  {
    "objectID": "src/introduction/kaplan.html#objectif",
    "href": "src/introduction/kaplan.html#objectif",
    "title": "Problème de Kaplan",
    "section": "Objectif",
    "text": "Objectif\nL’objectif du problème de Kaplan est d’estimer :\n\nLes paramètres : Les éléments des matrices A et B.\nLa condition initiale : Le vecteur d’état initial x_0.\n\nCes estimations sont basées sur des mesures bruitées de l’état x(t) et des entrées u(t) sur un intervalle de temps donné.",
    "crumbs": [
      "Plan du cours",
      "Introduction",
      "Problème de Kaplan"
    ]
  },
  {
    "objectID": "src/introduction/kaplan.html#importance",
    "href": "src/introduction/kaplan.html#importance",
    "title": "Problème de Kaplan",
    "section": "Importance",
    "text": "Importance\nLa résolution de ce problème est cruciale dans de nombreuses applications pratiques :\n\nContrôle de processus industriels : Pour ajuster les paramètres d’un système de contrôle afin d’optimiser la production.\nModélisation économique : Pour estimer les paramètres d’un modèle économique à partir de données historiques.\nBiologie : Pour comprendre les dynamiques de populations ou de réactions biochimiques.",
    "crumbs": [
      "Plan du cours",
      "Introduction",
      "Problème de Kaplan"
    ]
  },
  {
    "objectID": "src/introduction/kaplan.html#exemple-simple",
    "href": "src/introduction/kaplan.html#exemple-simple",
    "title": "Problème de Kaplan",
    "section": "Exemple Simple",
    "text": "Exemple Simple\nImaginons un système simple où un objet est soumis à une force de rappel proportionnelle à son déplacement (comme un ressort) et à une force d’amortissement proportionnelle à sa vitesse. L’équation différentielle pourrait être :\n\nx'(t) = \\begin{pmatrix} 0 & 1 \\\\ -k/m & -c/m \\end{pmatrix} x(t) + \\begin{pmatrix} 0 \\\\ 1/m \\end{pmatrix} u(t)\n\noù :\n\nx(t) = \\begin{pmatrix} x(t) \\\\ v(t) \\end{pmatrix} est le vecteur d’état contenant la position x(t) et la vitesse v(t).\nk est la constante de rappel du ressort.\nc est le coefficient d’amortissement.\nm est la masse de l’objet.\nu(t) est une force externe appliquée.\n\nDans ce contexte, le problème de Kaplan consisterait à estimer les valeurs de k, c, et m, ainsi que la condition initiale x(0), à partir de mesures de x(t) et u(t).",
    "crumbs": [
      "Plan du cours",
      "Introduction",
      "Problème de Kaplan"
    ]
  },
  {
    "objectID": "src/introduction/quiz.html#question-1",
    "href": "src/introduction/quiz.html#question-1",
    "title": "Quiz: Introduction aux Équations Différentielles Ordinaires",
    "section": "Question 1",
    "text": "Question 1\nQuelle est l’équation différentielle ordinaire d’ordre 1, scalaire, autonome et linéaire homogène ?\n\n\n\n\\(x'(t) = 10 x(t) + 2\\)\n\\(x'(t) = 5 x(t)\\)\n\\(x'(t) = t-x\\)\n\\(x''(t) = -x\\)"
  },
  {
    "objectID": "src/introduction/quiz.html#question-2",
    "href": "src/introduction/quiz.html#question-2",
    "title": "Quiz: Introduction aux Équations Différentielles Ordinaires",
    "section": "Question 2",
    "text": "Question 2\nDans l’équation \\(x'(t) = a x(t)\\), que représente \\(t\\) ?\n\n\n\nLa variable d’intégration\nLa variable inconnue\nUn paramètre fixé"
  },
  {
    "objectID": "src/introduction/quiz.html#question-3",
    "href": "src/introduction/quiz.html#question-3",
    "title": "Quiz: Introduction aux Équations Différentielles Ordinaires",
    "section": "Question 3",
    "text": "Question 3\nEst-ce que l’équation différentielle \\(x'(t) = A x(t) + B u(t)\\) est autonome ?\n\n\n\nOui\nNon"
  },
  {
    "objectID": "src/introduction/quiz.html#question-4",
    "href": "src/introduction/quiz.html#question-4",
    "title": "Quiz: Introduction aux Équations Différentielles Ordinaires",
    "section": "Question 4",
    "text": "Question 4\nPourquoi l’équation \\(x'(t) = a x(t)\\) est-elle qualifiée de scalaire ?\n\n\n\nParce qu’elle ne fait intervenir que la dérivée première de \\(x(t)\\)\nParce que \\(a\\) est scalaire\nParce que \\(x(t) \\in \\mathbb{R}\\)"
  },
  {
    "objectID": "src/introduction/quiz.html#question-5",
    "href": "src/introduction/quiz.html#question-5",
    "title": "Quiz: Introduction aux Équations Différentielles Ordinaires",
    "section": "Question 5",
    "text": "Question 5\nQuelle est la condition initiale pour l’équation \\(x'(t) = a x(t)\\) ?\n\n\n\n\\(x(0) = x_0\\)\n\\(x'(0) = x_0\\)\n\\(x(t) = x_0\\)"
  },
  {
    "objectID": "src/introduction/quiz.html#question-6",
    "href": "src/introduction/quiz.html#question-6",
    "title": "Quiz: Introduction aux Équations Différentielles Ordinaires",
    "section": "Question 6",
    "text": "Question 6\nQu’est-ce qu’un problème de Cauchy ?\n\n\n\nUn problème qui combine une équation différentielle avec une condition initiale\nUn problème qui combine une équation différentielle avec une condition aux limites\nUn problème qui combine une équation différentielle avec une condition de périodicité"
  },
  {
    "objectID": "src/introduction/quiz.html#question-7",
    "href": "src/introduction/quiz.html#question-7",
    "title": "Quiz: Introduction aux Équations Différentielles Ordinaires",
    "section": "Question 7",
    "text": "Question 7\nQuel théorème est utilisé pour montrer l’équivalence entre la formulation différentielle et la formulation intégrale d’une équation différentielle ?\n\n\n\nLe théorème fondamental de l’analyse\nLe théorème de Rolle\nLe théorème de la moyenne"
  },
  {
    "objectID": "src/introduction/quiz.html#question-8",
    "href": "src/introduction/quiz.html#question-8",
    "title": "Quiz: Introduction aux Équations Différentielles Ordinaires",
    "section": "Question 8",
    "text": "Question 8\nQuelle est la formulation intégrale de l’équation \\(x'(t) = a x(t)\\) avec la condition initiale \\(x(0) = x_0\\) ?\n\n\n\n\\(x(t) = x_0 + \\int_{0}^{t} a x(s) \\, \\mathrm{d}s\\)\n\\(x(t) = x_0 + \\int_{0}^{t} x(s) \\, \\mathrm{d}s\\)\n\\(x(t) = x_0 + \\int_{0}^{t} a \\, \\mathrm{d}s\\)"
  },
  {
    "objectID": "src/introduction/quiz.html#question-9",
    "href": "src/introduction/quiz.html#question-9",
    "title": "Quiz: Introduction aux Équations Différentielles Ordinaires",
    "section": "Question 9",
    "text": "Question 9\nQuelle est l’ordre de l’équation différentielle \\(x''(t) = -x(t)\\) ?\n\n\n\nOrdre 1\nOrdre 2\nOrdre 3"
  },
  {
    "objectID": "src/introduction/quiz.html#question-10",
    "href": "src/introduction/quiz.html#question-10",
    "title": "Quiz: Introduction aux Équations Différentielles Ordinaires",
    "section": "Question 10",
    "text": "Question 10\nPourquoi l’équation \\(x''(t) = -x(t)\\) est-elle qualifiée de scalaire ?\n\n\n\nParce qu’elle ne fait intervenir qu’une seule fonction inconnue \\(x(t)\\)\nParce qu’elle est linéaire\nParce qu’elle est autonome"
  },
  {
    "objectID": "src/introduction/quiz.html#question-11",
    "href": "src/introduction/quiz.html#question-11",
    "title": "Quiz: Introduction aux Équations Différentielles Ordinaires",
    "section": "Question 11",
    "text": "Question 11\nQuelle est la forme matricielle du système équivalent à l’équation \\(x''(t) = -x(t)\\) ?\n\n\n\n\\[\n\\begin{pmatrix}\nx'(t) \\\\\ny'(t)\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n0 & 1 \\\\\n-1 & 0\n\\end{pmatrix}\n\\begin{pmatrix}\nx(t) \\\\\ny(t)\n\\end{pmatrix}\n\\]\n\\[\n\\begin{pmatrix}\nx'(t) \\\\\ny'(t)\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n1 & 0 \\\\\n0 & 1\n\\end{pmatrix}\n\\begin{pmatrix}\nx(t) \\\\\ny(t)\n\\end{pmatrix}\n\\]\n\\[\n\\begin{pmatrix}\nx'(t) \\\\\ny'(t)\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n0 & 0 \\\\\n0 & 0\n\\end{pmatrix}\n\\begin{pmatrix}\nx(t) \\\\\ny(t)\n\\end{pmatrix}\n\\]"
  },
  {
    "objectID": "src/introduction/quiz.html#question-12",
    "href": "src/introduction/quiz.html#question-12",
    "title": "Quiz: Introduction aux Équations Différentielles Ordinaires",
    "section": "Question 12",
    "text": "Question 12\nQuelle est l’équation différentielle pour un point matériel soumis à la gravitation ?\n\n\n\n\\(x''(t) = g\\)\n\\(x''(t) = -x(t)\\)\n\\(x'(t) = a x(t)\\)"
  },
  {
    "objectID": "src/introduction/quiz.html#question-13",
    "href": "src/introduction/quiz.html#question-13",
    "title": "Quiz: Introduction aux Équations Différentielles Ordinaires",
    "section": "Question 13",
    "text": "Question 13\nQuelle est la forme matricielle du système équivalent à l’équation \\(x''(t) = g\\) ?\n\n\n\n\\[\\begin{pmatrix} x'(t) \\\\ v'(t) \\end{pmatrix} =\n\\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} \\begin{pmatrix} x(t) \\\\ v(t) \\end{pmatrix} +\n\\begin{pmatrix} 0 \\\\ g \\end{pmatrix}\\]\n\\[\\begin{pmatrix} x'(t) \\\\ v'(t) \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} \\begin{pmatrix} x(t) \\\\ v(t) \\end{pmatrix}\\]\n\\[\\begin{pmatrix} x'(t) \\\\ v'(t) \\end{pmatrix} = \\begin{pmatrix} 0 & 0 \\\\ 0 & 0 \\end{pmatrix} \\begin{pmatrix} x(t) \\\\ v(t) \\end{pmatrix}\\]"
  },
  {
    "objectID": "src/introduction/quiz.html#question-14",
    "href": "src/introduction/quiz.html#question-14",
    "title": "Quiz: Introduction aux Équations Différentielles Ordinaires",
    "section": "Question 14",
    "text": "Question 14\nQuelle est l’équation différentielle d’ordre 1 linéaire non homogène ?\n\n\n\n\\(x'(t) = A x(t) + B(t)\\)\n\\(x'(t) = A x(t)\\)\n\\(x''(t) = -x(t)\\)"
  },
  {
    "objectID": "src/introduction/quiz.html#question-15",
    "href": "src/introduction/quiz.html#question-15",
    "title": "Quiz: Introduction aux Équations Différentielles Ordinaires",
    "section": "Question 15",
    "text": "Question 15\nPourquoi les équations différentielles sont-elles importantes dans divers domaines scientifiques et techniques ?\n\n\n\nElles permettent de modéliser des systèmes dynamiques et de prédire leur comportement futur\nElles permettent de maximiser les profits\nElles permettent de minimiser les coûts"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n\n\n Back to top"
  },
  {
    "objectID": "src/introduction/edo.html",
    "href": "src/introduction/edo.html",
    "title": "Introduction",
    "section": "",
    "text": "Considérons pour commencer une équation différentielle ordinaire (EDO) d’ordre 1, scalaire, autonome et linéaire (homogène) :\n\nx'(t) = a x(t)\n\noù :\n\nt \\in \\mathbb{R} est le “temps”. C’est la variable d’intégration.\nt \\mapsto x(t) \\in \\mathbb{R} est la fonction inconnue que nous cherchons à déterminer. Elle représente l’état du système à l’instant t.\nt \\mapsto x'(t) est la dérivée de t \\mapsto x(t) par rapport au temps t.\na \\in \\mathbb{R} est un paramètre constant.\n\nDonnons de explications sur la terminologie employée.\n\nLe terme ordinaire est utilisé par opposition au terme équation différentielle partielle (plus communément équation aux dérivées partielles, ou EDP).\nDans ce cas, l’équation différentielle est scalaire car elle ne fait intervenir qu’une seule fonction inconnue t \\mapsto x(t) \\in \\mathbb{R}.\nElle est autonome car elle ne dépend pas explicitement du temps t.\nEnfin, elle est linéaire (homogène) car elle est linéaire en x(t).\nElle est d’ordre 1 car elle ne fait intervenir que la dérivée première de x notée x'(t), ou \\dot{x}(t) à la physicienne.",
    "crumbs": [
      "Plan du cours",
      "Introduction",
      "Équations différentielles ordinaires"
    ]
  },
  {
    "objectID": "src/introduction/edo.html#équations-différentielles-dordre-1-scalaire",
    "href": "src/introduction/edo.html#équations-différentielles-dordre-1-scalaire",
    "title": "Introduction",
    "section": "",
    "text": "Considérons pour commencer une équation différentielle ordinaire (EDO) d’ordre 1, scalaire, autonome et linéaire (homogène) :\n\nx'(t) = a x(t)\n\noù :\n\nt \\in \\mathbb{R} est le “temps”. C’est la variable d’intégration.\nt \\mapsto x(t) \\in \\mathbb{R} est la fonction inconnue que nous cherchons à déterminer. Elle représente l’état du système à l’instant t.\nt \\mapsto x'(t) est la dérivée de t \\mapsto x(t) par rapport au temps t.\na \\in \\mathbb{R} est un paramètre constant.\n\nDonnons de explications sur la terminologie employée.\n\nLe terme ordinaire est utilisé par opposition au terme équation différentielle partielle (plus communément équation aux dérivées partielles, ou EDP).\nDans ce cas, l’équation différentielle est scalaire car elle ne fait intervenir qu’une seule fonction inconnue t \\mapsto x(t) \\in \\mathbb{R}.\nElle est autonome car elle ne dépend pas explicitement du temps t.\nEnfin, elle est linéaire (homogène) car elle est linéaire en x(t).\nElle est d’ordre 1 car elle ne fait intervenir que la dérivée première de x notée x'(t), ou \\dot{x}(t) à la physicienne.",
    "crumbs": [
      "Plan du cours",
      "Introduction",
      "Équations différentielles ordinaires"
    ]
  },
  {
    "objectID": "src/introduction/edo.html#condition-initiale-et-problème-de-cauchy",
    "href": "src/introduction/edo.html#condition-initiale-et-problème-de-cauchy",
    "title": "Introduction",
    "section": "Condition Initiale et Problème de Cauchy",
    "text": "Condition Initiale et Problème de Cauchy\nL’équation différentielle x'(t) = a x(t) possède une infinité de solutions. Nous n’allons pas chercher l’ensemble des solutions, mais une solution particulière qui satisfait une condition initiale :\n\nx(0) = x_0.\n\nLe couple formé par l’équation différentielle et la condition initiale constitue un problème de Cauchy. La solution de ce problème est une fonction t \\mapsto x(t) qui satisfait à la fois l’équation différentielle et la condition initiale.\n\n\n\n\n\n\nNote\n\n\n\n\nNous noterons parfois x(\\cdot) la fonction inconnue t \\mapsto x(t) pour simplifier l’écriture.\nLa condition initiale x(0) = x_0 fixe la valeur de la fonction inconnue x(\\cdot) en t = 0. Elle permet de déterminer une solution unique au problème de Cauchy que nous considérons.\nNous avons fixé la condition initiale en t = 0 pour simplifier l’exposition. En pratique, la condition initiale peut être donnée à tout instant t = t_0.",
    "crumbs": [
      "Plan du cours",
      "Introduction",
      "Équations différentielles ordinaires"
    ]
  },
  {
    "objectID": "src/introduction/edo.html#lien-avec-la-formulation-intégrale",
    "href": "src/introduction/edo.html#lien-avec-la-formulation-intégrale",
    "title": "Introduction",
    "section": "Lien avec la Formulation Intégrale",
    "text": "Lien avec la Formulation Intégrale\nLe problème de Cauchy peut également être formulé sous la forme intégrale suivante :\n\nx(t) = x_0 + \\int_{0}^{t} a x(s) \\, \\mathrm{d}s.\n\nCette formulation intégrale est équivalente à la formulation différentielle. Il est important de retenir que même cette formulation réprésente une équation à résoudre dont l’inconnue est la fonction x(\\cdot).\n\n\n\n\n\n\nNote\n\n\n\nDe manière générale, pour montrer qu’une solution du problème de Cauchy est solution du problème sous forme intégrale, nous avons besoin du second théorème fondamental de l’analyse. Pour la réciproque, nous avons besoin du premier théorème fondamental de l’analyse.",
    "crumbs": [
      "Plan du cours",
      "Introduction",
      "Équations différentielles ordinaires"
    ]
  },
  {
    "objectID": "src/introduction/edo.html#espace-des-solutions",
    "href": "src/introduction/edo.html#espace-des-solutions",
    "title": "Introduction",
    "section": "Espace des Solutions",
    "text": "Espace des Solutions\nPour une équation différentielle, comme pour toute équation, il est crucial de définir l’espace dans lequel nous cherchons les solutions. Ici, nous cherchons une fonction x(\\cdot) qui est :\n\nDérivable\nDéfinie sur un intervalle de temps ouvert contenant 0 (car la condition initiale est donnée en t = 0).",
    "crumbs": [
      "Plan du cours",
      "Introduction",
      "Équations différentielles ordinaires"
    ]
  },
  {
    "objectID": "src/introduction/edo.html#équation-différentielle-dordre-2-scalaire",
    "href": "src/introduction/edo.html#équation-différentielle-dordre-2-scalaire",
    "title": "Introduction",
    "section": "Équation Différentielle d’Ordre 2 Scalaire",
    "text": "Équation Différentielle d’Ordre 2 Scalaire\nConsidérons maintenant une équation différentielle d’ordre 2 :\n\nx''(t) = -x(t)\n\noù x''(t) représente la dérivée seconde de x(\\cdot) au temps t. Cette équation est d’ordre 2 car elle fait intervenir la dérivée seconde de x(\\cdot). Elle est scalaire car elle ne fait intervenir qu’une seule fonction inconnue x(\\cdot).\nPour ramener cette équation à un système d’ordre 1, nous introduisons une nouvelle variable y(t) = x'(t). Ainsi, nous obtenons le système suivant :\n\n\\begin{cases}\nx'(t) = y(t), \\\\\ny'(t) = -x(t).\n\\end{cases}\n\nCe système est équivalent à l’équation d’ordre 2 initiale et peut être résolu en utilisant les techniques d’ordre 1. Par la suite, nous ne considérons que des équations d’ordre 1, scalaires ou vectorielles. L’équation précédente, dans les coordonnées (x, y), est une équation différentielle ordinaire d’ordre 1 linéaire, autonome et vectorielle. Elle peut s’écrire sous forme matricielle :\n\n\\begin{pmatrix}\nx'(t) \\\\\ny'(t)\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n0 & 1 \\\\\n-1 & 0\n\\end{pmatrix}\n\\begin{pmatrix}\nx(t) \\\\\ny(t)\n\\end{pmatrix}.\n\n\nExemple\nConsidérons un point matériel soumis à la gravitation, où l’équation différentielle est donnée par :\n\nx''(t) = g\n\noù g est l’accélération due à la gravité. Cette équation est d’ordre 2 et scalaire.\nPour ramener cette équation à un système d’ordre 1, nous introduisons une nouvelle variable v(t) = x'(t), représentant la vitesse du point matériel. Ainsi, nous obtenons le système suivant :\n\n\\begin{cases}\nx'(t) = v(t), \\\\\nv'(t) = g.\n\\end{cases}\n\nCe système est équivalent à l’équation d’ordre 2 initiale et peut être résolu en utilisant les techniques d’ordre 1. Le système peut s’écrire sous forme matricielle :\n\n\\begin{pmatrix}\nx'(t) \\\\\nv'(t)\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n0 & 1 \\\\\n0 & 0\n\\end{pmatrix}\n\\begin{pmatrix}\nx(t) \\\\\nv(t)\n\\end{pmatrix}\n+\n\\begin{pmatrix}\n0 \\\\\ng\n\\end{pmatrix}\n\nCet exemple illustre comment une équation différentielle d’ordre 2 peut être transformée en un système d’ordre 1 pour faciliter sa résolution.",
    "crumbs": [
      "Plan du cours",
      "Introduction",
      "Équations différentielles ordinaires"
    ]
  },
  {
    "objectID": "src/introduction/edo.html#équations-différentielles-dordre-1-vectorielle",
    "href": "src/introduction/edo.html#équations-différentielles-dordre-1-vectorielle",
    "title": "Introduction",
    "section": "Équations Différentielles d’Ordre 1 Vectorielle",
    "text": "Équations Différentielles d’Ordre 1 Vectorielle\nConsidérons maintenant une équation différentielle ordinaire d’ordre 1 linéaire non homogène :\n\nx'(t) = A x(t) + b(t)\n\noù A est une matrice constante de taille n \\times n et b(t) est un vecteur de taille n dépendant du temps t.\nCette équation n’est pas autonome car elle dépend explicitement du temps t à travers b(t). Cette équation est affine en x(t), car elle contient un terme linéaire A x(t) et un terme constant b(t). Nous parlerons d’équation différentielle linéaire non homogène. C’est une équation d’ordre 1 car elle ne fait intervenir que la dérivée première de x(t), mais c’est une équation vectorielle car x(t) est un vecteur de taille n.\n\n\n\n\n\n\nNote\n\n\n\nToutes les équations différentielles que nous considérons seront linéaires et auront une matrice A constante. Cela simplifie la résolution.\n\n\nPour mieux comprendre que le temps t intervient explicitement seulement dans b(t), nous pouvons réécrire l’équation à l’aide d’une fonction f (appelée second membre) dépendant du temps t et de la variable x :\n\nx'(t) = f(t, x(t))\n\noù f est une fonction de \\mathbb{R} \\times \\mathbb{R}^n dans \\mathbb{R}^n définie par\n\nf(t, x) = A x + b(t).\n\n\n\n\n\n\n\nNote\n\n\n\n\nOn dira que l’équation différentielle est autonome si f ne dépend pas explicitement du temps t et non autonome sinon.\nDans la littérature, on parle d’équation différentielle linéaire homogène lorsque b(t) = 0 et d’équation différentielle linéaire non homogène (ou avec second membre) sinon.",
    "crumbs": [
      "Plan du cours",
      "Introduction",
      "Équations différentielles ordinaires"
    ]
  },
  {
    "objectID": "src/introduction/edo.html#importance-des-équations-différentielles",
    "href": "src/introduction/edo.html#importance-des-équations-différentielles",
    "title": "Introduction",
    "section": "Importance des Équations Différentielles",
    "text": "Importance des Équations Différentielles\nLes équations différentielles sont fondamentales dans de nombreux domaines scientifiques et techniques. Elles permettent de modéliser des systèmes dynamiques et de prédire leur comportement futur. Par exemple :\n\nEn physique, elles décrivent le mouvement des objets.\nEn biologie, elles modélisent la croissance des populations.\nEn économie, elles analysent les fluctuations du marché.\n\nEn maîtrisant les concepts et les techniques de résolution des équations différentielles, nous pouvons mieux comprendre et prédire le comportement de ces systèmes complexes.",
    "crumbs": [
      "Plan du cours",
      "Introduction",
      "Équations différentielles ordinaires"
    ]
  },
  {
    "objectID": "src/introduction/edo.html#quiz",
    "href": "src/introduction/edo.html#quiz",
    "title": "Introduction",
    "section": "Quiz",
    "text": "Quiz",
    "crumbs": [
      "Plan du cours",
      "Introduction",
      "Équations différentielles ordinaires"
    ]
  },
  {
    "objectID": "src/introduction/moindres_carres.html",
    "href": "src/introduction/moindres_carres.html",
    "title": "Problème de Kaplan sous Forme de Moindres Carrés",
    "section": "",
    "text": "Le problème de Kaplan peut être formulé comme un problème d’optimisation aux moindres carrés. L’objectif est d’estimer les paramètres et la condition initiale d’une équation différentielle linéaire vectorielle en minimisant l’écart entre les mesures observées et les prédictions du modèle.",
    "crumbs": [
      "Plan du cours",
      "Introduction",
      "Problème aux moindres carrés"
    ]
  },
  {
    "objectID": "src/introduction/moindres_carres.html#formulation-mathématique",
    "href": "src/introduction/moindres_carres.html#formulation-mathématique",
    "title": "Problème de Kaplan sous Forme de Moindres Carrés",
    "section": "Formulation Mathématique",
    "text": "Formulation Mathématique\nConsidérons le problème de Cauchy linéaire décrit par :\n\nx'(t) = Ax(t) + Bu(t), \\quad x(0) = x_0\n\noù :\n\nx(t) \\in \\mathbb{R}^n est le vecteur d’état à l’instant t.\nA \\in \\mathrm{M}_{n}(\\mathbb{R}) est la matrice des coefficients.\nB \\in \\mathrm{M}_{n, m}(\\mathbb{R}) est la matrice d’entrée.\nu(t) \\in \\mathbb{R}^m est le vecteur des entrées.\n\nNous disposons de mesures x_i à des instants t_i pour i = 1, 2, \\ldots, N. Nous cherchons à estimer les paramètres\n\n   \\theta = (A, B, x_0)\n en minimisant la fonction coût suivante :\n\nF(\\theta) = \\frac{1}{2} \\sum_{i=1}^{N} \\| x_i - x(t_i, \\theta) \\|^2\n\noù :\n\nx(t_i, \\theta) est la prédiction du modèle, c’est-à-dire la solution du problème de Cauchy, à l’instant t_i avec les paramètres \\theta.\n\\| \\cdot \\| désigne la norme euclidienne.\n\n\n\n\n\n\n\nNote\n\n\n\n\nLa fonction coût F est la somme des carrés des erreurs entre les mesures observées x_i et les prédictions du modèle x(t_i, \\theta). On appelle ces erreurs les résidus.\nL’objectif est de trouver les paramètres \\theta qui minimisent cette fonction coût.\nCette formulation est un problème d’optimisation aux moindres carrés.\nLes mesures x_i peuvent être bruitées, ce qui nécessite une approche robuste pour estimer les paramètres.\n\n\n\nLes matrices A et B et le vecteur x_0 sont ici considérés comme des paramètres à estimer. Dans certains cas, seuls certains paramètres peuvent être inconnus. Il est aussi possible de ne chercher qu’à estimer une partie des paramètres. Dans ce cas, si nous notons \\theta \\in \\mathbb{R}^l les paramètres à estimer, nous pouvons écrire la dépendance de A, B et x_0 en fonction de \\theta, et le problème de Cauchy devient :\n\nx'(t) = A(\\theta)\\, x(t) + B(\\theta)\\, u(t), \\quad x(0) = x_0(\\theta).\n\n\n\n\n\n\n\nImportant\n\n\n\nNous parlons de problème aux moindres carrés linéaire si \\theta \\mapsto x(t_i, \\theta) est linéaire pour toute donnée t_i, sinon on parle de problème aux moindres carrés non linéaire.",
    "crumbs": [
      "Plan du cours",
      "Introduction",
      "Problème aux moindres carrés"
    ]
  },
  {
    "objectID": "src/introduction/moindres_carres.html#algorithme-de-gauss-newton",
    "href": "src/introduction/moindres_carres.html#algorithme-de-gauss-newton",
    "title": "Problème de Kaplan sous Forme de Moindres Carrés",
    "section": "Algorithme de Gauss-Newton",
    "text": "Algorithme de Gauss-Newton\n\nDescription de la méthode\nL’algorithme de Gauss-Newton est une méthode itérative utilisée pour minimiser la fonction coût F. Cette algorithme est utilisé pour la résolution de problèmes aux moindres carrés non linéaires. Si le problème est linéaire, seul la première itération suffit. A partir d’un itéré initial \\theta_0, l’algorithme va construire une suite d’itérés (\\theta_k)_{k\\ge 0} qui on l’espère converge vers un minimum de F. L’idée principale de l’alogrithme de Gauss-Newton est de résoudre à chaque itération un problème aux moindres carrés linéaire. Notons \\theta_k l’itéré courant. Nous pouvons approcher pour chaque instant t_i la prédiction \\theta \\mapsto x(t_i, \\theta) par son approximation linéaire au voisinage du point courant \\theta_k :\n\n   x(t_i, \\theta_k) + \\frac{\\partial x}{\\partial \\theta}(t_i, \\theta_k)\\, \\Delta \\theta.\n\nCeci nous amène, à chaque itération, à devoir résoudre un problème aux moindres carrés linéaires :\n\n   \\mathrm{minimiser}~ F_k(\\Delta \\theta) = \\frac{1}{2} \\sum_{i=1}^{N} \\left\\| x_i - x(t_i, \\theta_k) - \\frac{\\partial x}{\\partial \\theta}(t_i, \\theta_k)\\, \\Delta \\theta \\right\\|^2.\n\n\n\n\n\n\n\nImportant\n\n\n\nA chaque itération, la fonction à minimiser dépend de \\Delta \\theta, la valeur du paramètre \\theta est fixée à \\theta_k.\n\n\nLe prochain itéré s’écrit :\n\n\\theta_{k+1} = \\theta_k + \\Delta \\theta_k\n\noù \\Delta \\theta_k est la solution du problème aux moindres carrés linéaires faisant intervenir F_k.\n\n\nSolution du problème linéaire\nLe point \\Delta \\theta_k \\in \\mathbb{R}^l est obtenu en annulant le gradient de F_k, c’est-à-dire en résolvant :\n\n\\begin{aligned}\n   \\nabla F_k(\\Delta \\theta) &= \\sum_{i=1}^{N} {\\frac{\\partial x}{\\partial \\theta}(t_i, \\theta_k)}^T \\left( x_i - x(t_i, \\theta_k) - \\frac{\\partial x}{\\partial \\theta}(t_i, \\theta_k)\\, \\Delta \\theta \\right) \\\\\n   &= \\sum_{i=1}^{N} {J_{i,k}}^T \\left( x_i - x(t_i, \\theta_k) \\right) - \\left( \\sum_{i=1}^{N} {J_{i,k}}^T J_{i,k} \\right) \\Delta \\theta = 0,\n\\end{aligned}\n\noù l’on a introduit la notation\n\n   J_{i,k} = \\frac{\\partial x}{\\partial \\theta}(t_i, \\theta_k).\n\nSi l’on regarde attentivement l’équation ci-dessus en \\Delta \\theta, nous pouvons remarquer que nous devons résoudre un simple système linéaire de la forme :\n\n   A_k \\Delta \\theta = b_k,\n\noù\n\n   A_k = \\sum_{i=1}^{N} {J_{i,k}}^T J_{i,k} \\in \\mathrm{M}_{l}(\\mathbb{R})\n   \\quad \\text{et} \\quad\n   b_k = \\sum_{i=1}^{N} {J_{i,k}}^T \\left( x_i - x(t_i, \\theta_k) \\right) \\in \\mathbb{R}^l.\n\n\n\nPrésentation de l’algorithme\nVoici les étapes principales :\n\nInitialisation : Choisir une estimation initiale \\theta_0 des paramètres.\nItération : Pour chaque itération :\n\nCalculer la prédiction x(t_i, \\theta_k) pour chaque instant t_i.\nCalculer les matrices jacobiennes J_{i,k} des dérivées partielles de x(t_i, \\theta) par rapport à \\theta, évaluée en \\theta_k :\n\n  J_{i,k} = \\frac{\\partial x}{\\partial \\theta}(t_i, \\theta_k) =\n   \\begin{pmatrix}\n    \\displaystyle\\frac{\\partial x_1}{\\partial \\theta_1}(t_i, \\theta_k) &\n    \\displaystyle\\frac{\\partial x_1}{\\partial \\theta_2}(t_i, \\theta_k) &\n    \\displaystyle\\ldots &\n    \\displaystyle\\frac{\\partial x_1}{\\partial \\theta_l}(t_i, \\theta_k) \\\\[1em]\n    \\displaystyle\\frac{\\partial x_2}{\\partial \\theta_1}(t_i, \\theta_k) &\n    \\displaystyle\\frac{\\partial x_2}{\\partial \\theta_2}(t_i, \\theta_k) &\n    \\displaystyle\\ldots &\n    \\displaystyle\\frac{\\partial x_2}{\\partial \\theta_l}(t_i, \\theta_k) \\\\[1em]\n    \\displaystyle\\vdots &\n    \\displaystyle\\vdots &\n    \\displaystyle\\ddots &\n    \\displaystyle\\vdots \\\\[1em]\n    \\displaystyle\\frac{\\partial x_n}{\\partial \\theta_1}(t_i, \\theta_k) &\n    \\displaystyle\\frac{\\partial x_n}{\\partial \\theta_2}(t_i, \\theta_k) &\n    \\displaystyle\\ldots &\n    \\displaystyle\\frac{\\partial x_n}{\\partial \\theta_l}(t_i, \\theta_k)\n  \\end{pmatrix} \\in \\mathrm{M}_{n, l}(\\mathbb{\\R})\n  \nMettre à jour les paramètres en utilisant la formule :\n\n\\theta_{k+1} = \\theta_k + \\Delta \\theta_k\n\noù \\Delta \\theta_k est la solution du système linéaire :\n\nA_k \\Delta \\theta = b_k\n\navec A_k et b_k définis ci-dessus.\n\nConvergence : Répéter jusqu’à ce que les changements dans \\theta soient négligeables ou qu’un critère de convergence soit satisfait.",
    "crumbs": [
      "Plan du cours",
      "Introduction",
      "Problème aux moindres carrés"
    ]
  },
  {
    "objectID": "src/introduction/moindres_carres.html#fonctions-nécessaires",
    "href": "src/introduction/moindres_carres.html#fonctions-nécessaires",
    "title": "Problème de Kaplan sous Forme de Moindres Carrés",
    "section": "Fonctions Nécessaires",
    "text": "Fonctions Nécessaires\nPour implémenter cet algorithme, deux fonctions principales sont nécessaires :\n\nFonction de Prédiction :\n\nEntrée : Paramètre \\theta et instant t_i.\nSortie : Prédictions x(t_i, \\theta) en résolvant le problème de Cauchy jusqu’au temps t_i.\n\nFonction de Dérivée :\n\nEntrée : Paramètre \\theta et instant t_i.\nSortie : Matrice jacobienne des dérivées partielles de x(t_i, \\theta) par rapport à \\theta.\n\n\nCes fonctions permettent de calculer les prédictions du modèle et les dérivées nécessaires pour mettre à jour les paramètres à chaque itération de l’algorithme de Gauss-Newton. La suite du cours est consacrée à l’implémentation de ces fonctions pour résoudre des problèmes de Kaplan concrets.",
    "crumbs": [
      "Plan du cours",
      "Introduction",
      "Problème aux moindres carrés"
    ]
  },
  {
    "objectID": "src/introduction/moindres_carres.html#exercice",
    "href": "src/introduction/moindres_carres.html#exercice",
    "title": "Problème de Kaplan sous Forme de Moindres Carrés",
    "section": "Exercice",
    "text": "Exercice\nConsidérons le modèle linéaire suivant :\n x(t, \\theta) = a + b\\, t \noù :\n\nx(t, \\theta) est la sortie du modèle à l’instant t pour les paramètres \\theta.\n\\theta = (a, b) sont les paramètres à estimer.\n\nSupposons que nous avons les observations suivantes :\n\n\\begin{array}{c|c}\nt & x \\\\\n\\hline\n1 & 2 \\\\\n2 & 3 \\\\\n3 & 5 \\\\\n\\end{array}\n\nNous cherchons à estimer les paramètres a et b en minimisant la somme des carrés des résidus entre les observations et les prédictions du modèle.\n\n\n\n\n\n\nNote\n\n\n\nNous pouvons voir x(t, \\theta) comme la solution du problème de Cauchy : \n   x'(t) = b, \\quad x(0) = a.\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nDévelopper à la main la première itération de l’algorithme de Gauss-Newton. Vous pouvez choisir \\theta_0 = (1, 1) comme initialisation.\n\n\n\n\n\n\n\n\nCorrection\n\n\n\n\n\nNous cherchons à estimer les paramètres a et b en minimisant la somme des carrés des résidus entre les observations et les prédictions du modèle.\n\nÉtapes de l’algorithme de Gauss-Newton\n\nInitialisation :\n\nChoisissons une estimation initiale \\theta_0 = (1, 1).\n\nItération 1 :\n\nCalculons les prédictions x(t_i, \\theta_0) pour chaque instant t_i : \nx(1, \\theta_0) = 1 \\cdot 1 + 1 \\cdot 1 = 2\n \nx(2, \\theta_0) = 1 \\cdot 1 + 1 \\cdot 2 = 3\n \nx(3, \\theta_0) = 1 \\cdot 1 + 1 \\cdot 3 = 4\n\nCalculons la matrice jacobienne J_{i,0} des dérivées partielles de x(t_i, \\theta) par rapport à \\theta, évaluée en \\theta_0 : \nJ_{i,0} = \\begin{pmatrix}\n\\displaystyle\\frac{\\partial x}{\\partial a}(t_i, \\theta_0) &\n\\displaystyle\\frac{\\partial x}{\\partial b}(t_i, \\theta_0)\n\\end{pmatrix}\n= \\begin{pmatrix}\n1 & t_i\n\\end{pmatrix}\n Donc, \nJ_{1,0} = \\begin{pmatrix} 1 & 1 \\end{pmatrix}, \\quad J_{2,0} = \\begin{pmatrix} 1 & 2 \\end{pmatrix}, \\quad J_{3,0} = \\begin{pmatrix} 1 & 3 \\end{pmatrix}\n\nRésolvons le système linéaire pour obtenir \\Delta \\theta_0 : \nA_0 \\Delta \\theta = b_0\n où \nA_0 = \\sum_{i=1}^{3} {J_{i,0}}^T J_{i,0} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} \\begin{pmatrix} 1 & 1 \\end{pmatrix} + \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} \\begin{pmatrix} 1 & 2 \\end{pmatrix} + \\begin{pmatrix} 1 \\\\ 3 \\end{pmatrix} \\begin{pmatrix} 1 & 3 \\end{pmatrix}\n= \\begin{pmatrix} 3 & 6 \\\\ 6 & 14 \\end{pmatrix}\n et \nb_0 = \\sum_{i=1}^{3} {J_{i,0}}^T \\left( x_i - x(t_i, \\theta_0) \\right) = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} \\cdot 0 + \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} \\cdot 0 + \\begin{pmatrix} 1 \\\\ 3 \\end{pmatrix} \\cdot 1 = \\begin{pmatrix} 1 \\\\ 3 \\end{pmatrix}\n Donc, \n\\begin{pmatrix} 3 & 6 \\\\ 6 & 14 \\end{pmatrix} \\Delta \\theta = \\begin{pmatrix} 1 \\\\ 3 \\end{pmatrix}\n En résolvant ce système, nous obtenons : \n\\Delta \\theta_0 = \\begin{pmatrix} -2/3 \\\\ 0.5 \\end{pmatrix}\n\nMettons à jour les paramètres : \n\\theta_1 = \\theta_0 + \\Delta \\theta_0 = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} + \\begin{pmatrix} -2/3 \\\\ 0.5 \\end{pmatrix} = \\begin{pmatrix} 1/3 \\\\ 1.5 \\end{pmatrix}\n\n\nConvergence :\n\nRépéter les étapes jusqu’à ce que les changements dans \\theta soient négligeables ou qu’un critère de convergence soit satisfait.\n\n\n\n\nTableau de Convergence\nOn rappelle que le paramètre s’écrit \\theta = (a, b). On note alors la variation d’une itération à l’autre \\Delta \\theta = (\\Delta a, \\Delta b). Puisque le problème est déjà un problème aux moindres carrés linéaires, nous n’observons aucune amélioration après la première itération, l’algorithme converge en une seule itération.\n\n\n\n\n\n\n\n\n\n\n\nItération\na\nb\nÉcart \\Delta a\nÉcart \\Delta b\nValeur du coût F(\\theta)\n\n\n\n\n0\n1.000\n1.000\n-\n-\n0.500\n\n\n1\n1/3\n1.500\n-2/3\n0.500\n0.083\n\n\n2\n1/3\n1.500\n0.000\n0.000\n0.083\n\n\n\nCet exemple montre comment appliquer l’algorithme de Gauss-Newton à la main pour estimer les paramètres d’un modèle linéaire simple. Le tableau de convergence permet de visualiser la diminution de l’écart entre les itérations et la minimisation du coût.\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNous verrons en TP comment résoudre numériquement ce problème en Python.",
    "crumbs": [
      "Plan du cours",
      "Introduction",
      "Problème aux moindres carrés"
    ]
  },
  {
    "objectID": "src/edo_lineaires/definition.html",
    "href": "src/edo_lineaires/definition.html",
    "title": "Problème de Cauchy Linéaire",
    "section": "",
    "text": "Un problème de Cauchy linéaire consiste à résoudre une équation différentielle ordinaire (EDO) linéaire accompagnée d’une condition initiale. Il s’écrit sous la forme :\n\nx'(t) = A(t)\\, x(t) + b(t), \\quad x(t_0) = x_0\n\noù :\n\nx\\colon I \\to \\mathbb{R}^n est la fonction inconnue définie sur un intervalle I contenant t_0,\nA(t) \\in \\mathrm{M}_{n}(\\mathbb{R}) est une matrice donnée, dépendant du temps t,\nb(t) \\in \\mathbb{R}^n est un vecteur donné, dépendant du temps t,\nt_0 \\in I est l’instant initial,\nx_0 \\in \\mathbb{R}^n est la condition initiale.\n\n\n\n\n\n\n\nNote\n\n\n\nNous considérons ici le cas linéaire, et dans la suite du cours, nous ne traiterons que le cas où A(t) est constant.",
    "crumbs": [
      "Plan du cours",
      "Problème de Cauchy linéaire",
      "Définition"
    ]
  },
  {
    "objectID": "src/edo_lineaires/definition.html#définition",
    "href": "src/edo_lineaires/definition.html#définition",
    "title": "Problème de Cauchy Linéaire",
    "section": "",
    "text": "Un problème de Cauchy linéaire consiste à résoudre une équation différentielle ordinaire (EDO) linéaire accompagnée d’une condition initiale. Il s’écrit sous la forme :\n\nx'(t) = A(t)\\, x(t) + b(t), \\quad x(t_0) = x_0\n\noù :\n\nx\\colon I \\to \\mathbb{R}^n est la fonction inconnue définie sur un intervalle I contenant t_0,\nA(t) \\in \\mathrm{M}_{n}(\\mathbb{R}) est une matrice donnée, dépendant du temps t,\nb(t) \\in \\mathbb{R}^n est un vecteur donné, dépendant du temps t,\nt_0 \\in I est l’instant initial,\nx_0 \\in \\mathbb{R}^n est la condition initiale.\n\n\n\n\n\n\n\nNote\n\n\n\nNous considérons ici le cas linéaire, et dans la suite du cours, nous ne traiterons que le cas où A(t) est constant.",
    "crumbs": [
      "Plan du cours",
      "Problème de Cauchy linéaire",
      "Définition"
    ]
  },
  {
    "objectID": "src/edo_lineaires/definition.html#terminologie",
    "href": "src/edo_lineaires/definition.html#terminologie",
    "title": "Problème de Cauchy Linéaire",
    "section": "Terminologie",
    "text": "Terminologie\nSoit l’équation différentielle ordinaire linéaire :\n\nx'(t) = A(t)\\, x(t) + b(t),\n\noù A(t) \\in \\mathrm{M}_{n}(\\mathbb{R}) et b(t) \\in \\mathbb{R}^n. On distingue plusieurs cas :\n\nÉquation linéaire homogène : lorsque b(t) = 0, l’équation devient x'(t) = A(t)\\, x(t).\nÉquation linéaire non homogène (ou avec second membre) : lorsque b(t) \\neq 0.\nÉquation autonome : lorsque A(t) et b(t) sont constants, c’est-à-dire indépendants de t.",
    "crumbs": [
      "Plan du cours",
      "Problème de Cauchy linéaire",
      "Définition"
    ]
  },
  {
    "objectID": "src/edo_lineaires/definition.html#solution",
    "href": "src/edo_lineaires/definition.html#solution",
    "title": "Problème de Cauchy Linéaire",
    "section": "Solution",
    "text": "Solution\n\n\n\n\n\n\nDéfinition\n\n\n\nOn suppose les fonctions t \\mapsto A(t) et t \\mapsto b(t) définies et continues sur un intervalle ouvert \\mathcal{I} contenant t_0.\nOn appelle solution du problème de Cauchy associé, tout couple (I, \\varphi), où I est un intervalle ouvert de \\mathcal{I} contenant t_0 et \\varphi \\colon I \\to \\mathbb{R}^n est une fonction dérivable sur I, tel que pour tout t \\in I, \\varphi'(t) = A(t)\\, \\varphi(t) + b(t) et \\varphi(t_0) = x_0.\n\n\n\n\n\n\n\n\nNote\n\n\n\nIl est courant d’utiliser la même notation pour la solution \\varphi et l’inconnue x.\n\n\nUne solution (I,\\varphi) est dite maximale si, pour toute autre solution (J,\\psi), on a J \\subset I et \\varphi = \\psi sur J. On dit que qu’une solution (I,\\varphi) est un prolongement d’une autre solution (J,\\psi), si J \\subset I et \\varphi = \\psi sur J. On parle de solution globale si ell est définie sur tout l’intervalle \\mathcal{I}.\n\n\n\n\n\n\nNote\n\n\n\n\nToute solution se prolonge en une solution maximale (pas nécessairement unique).\nTout solution globale est maximale mais pas l’inverse.",
    "crumbs": [
      "Plan du cours",
      "Problème de Cauchy linéaire",
      "Définition"
    ]
  },
  {
    "objectID": "src/edo_lineaires/definition.html#formulation-intégrale",
    "href": "src/edo_lineaires/definition.html#formulation-intégrale",
    "title": "Problème de Cauchy Linéaire",
    "section": "Formulation Intégrale",
    "text": "Formulation Intégrale\n\n\n\n\n\n\nThéorème\n\n\n\nSoit un couple (I, \\varphi), avec I un intervalle ouvert de \\mathcal{I} et \\varphi \\colon I \\to \\mathbb{R}^n une fonction dérivable sur I.\nAlors, le couple (I, \\varphi) est solution du problème de Cauchy si et seulement si pour tout t \\in I on a :\n\n\\varphi(t) = x_0 + \\int_{t_0}^{t} \\left( A(s)\\, \\varphi(s) + b(s) \\right) \\, \\mathrm{d}s.\n\n\n\n\n\n\n\n\n\nPreuve\n\n\n\n\n\nNotons f \\colon \\mathcal{I} \\times \\mathbb{R}^n \\to \\mathbb{R}^n, (t, x) \\mapsto f(t, x) \\coloneqq A(t)\\, x + b(t).\n\\Rightarrow Puisque \\varphi est dérivable sur I, l’application t \\mapsto g(t) \\coloneqq f(t, \\varphi(t)) est continue sur I, donc intégrable sur tout compact de I. De plus, \\varphi est une primitive de g sur I, et d’après le second théorème fondamental de l’analyse, on obtient : \n  \\int_{t_0}^{t} g(s) \\, \\mathrm{d}s = \\varphi(t) - \\varphi(t_0),\n ce qui entraîne : \n  \\int_{t_0}^{t} f(s, \\varphi(s)) \\, \\mathrm{d}s = \\varphi(t) - x_0.\n\n\\Leftarrow L’application g est intégrable sur tout compact de I, donc la fonction \nG(t) \\coloneqq \\int_{t_0}^{t} g(s) \\, \\mathrm{d}s\n est bien définie pour tout t \\in I. Comme g est continue, G est l’unique primitive de g s’annulant en t_0 (d’après le premier théorème fondamental de l’analyse). Ainsi, si pour tout t \\in I : \n\\varphi(t) = x_0 + \\int_{t_0}^{t} \\left( A(s)\\, \\varphi(s) + b(s) \\right) \\, \\mathrm{d}s = x_0 + G(t),\n alors on a \\varphi(t_0) = x_0 (puisque G(t_0) = 0). En différenciant les deux membres de cette expression, on obtient : \n\\varphi'(t) = G'(t) = f(t, \\varphi(t)),\n ce qui achève la démonstration.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nCette formulation intégrale est donc équivalente à la formulation différentielle. Il est important de retenir que même cette formulation réprésente une équation à résoudre.",
    "crumbs": [
      "Plan du cours",
      "Problème de Cauchy linéaire",
      "Définition"
    ]
  },
  {
    "objectID": "src/edo_lineaires/definition.html#cas-scalaire",
    "href": "src/edo_lineaires/definition.html#cas-scalaire",
    "title": "Problème de Cauchy Linéaire",
    "section": "Cas Scalaire",
    "text": "Cas Scalaire\nDans le cas d’une équation linéaire homogène scalaire autonome, où a est une fonction constante, et en considérant le problème de Cauchy avec la condition initiale x(0) = x_0, l’équation est donnée par :\n\nx'(t) = a x(t), \\quad x(0) = x_0.\n\nLa solution générale de l’équation différentielle est :\n\nx(t) = C e^{a t},\n\noù C est une constante déterminée par la condition initiale. En imposant x(0) = x_0, on trouve :\n\nC = x_0.\n\nAinsi, la solution du problème de Cauchy est :\n\nx(t) = x_0 e^{a t}.\n\n\n\n\n\n\n\nCommentaire sur le Cas Vectoriel\n\n\n\nLe cas scalaire fait intervenir la fonction exponentielle sur \\mathbb{R}. Dans le cas vectoriel, où x(t) \\in \\mathbb{R}^n, nous aurons besoin de l’exponentielle de matrice, qui est une généralisation de l’exponentielle scalaire.",
    "crumbs": [
      "Plan du cours",
      "Problème de Cauchy linéaire",
      "Définition"
    ]
  },
  {
    "objectID": "src/edo_lineaires/solution.html",
    "href": "src/edo_lineaires/solution.html",
    "title": "Solution du Problème de Cauchy Linéaire",
    "section": "",
    "text": "Considérons le problème de Cauchy linéaire homogène suivant :\n\n    x'(t) = A\\, x(t), \\quad x(0) = x_0\n\noù A \\in M_n(\\mathbb{R}) est une matrice constante et x_0 \\in \\mathbb{R}^n est le vecteur d’état initial.\n\n\n\n\n\n\nThéorème\n\n\n\nLa solution du problème de Cauchy linéaire homogène est donnée par :\n\n    x(t, x_0) = e^{tA} x_0\n\n\n\n\n\n\n\n\n\nPreuve\n\n\n\n\n\nExistence. Montrons que x(t, x_0) = e^{tA} x_0 vérifie les propriétés d’une solution pour le problème de Cauchy.\n\nVérification de la Condition Initiale. À t = 0, nous avons : \n      x(0, x_0) = e^{0 \\cdot A} x_0 = I x_0 = x_0\n   Donc, la condition initiale est satisfaite.\nVérification de l’Équation Différentielle. Calculons la dérivée de x(t, x_0) par rapport à t : \n      \\frac{d}{dt} x(t, x_0) = \\frac{d}{dt} (e^{tA} x_0) = A e^{tA} x_0 = A x(t, x_0)\n   Donc, x(t, x_0) satisfait l’équation différentielle x'(t) = A x(t).\n\nAinsi, x(t, x_0) = e^{tA} x_0 est bien une solution du problème de Cauchy, ce qui prouve l’existence.\nUnicité. Supposons qu’il existe une autre solution y(t) du problème de Cauchy. Définissons z(t) = y(t) - x(t, x_0). Alors,\n\n    z'(t) = y'(t) - x'(t, x_0) = A y(t) - A x(t, x_0) = A (y(t) - x(t, x_0)) = A z(t)\n\nAvec la condition initiale z(0) = y(0) - x(0, x_0) = 0.\nConsidérons maintenant le changement de variable w(t) = e^{-tA} z(t). Alors,\n\n    w'(t) = \\frac{d}{dt} (e^{-tA} z(t)) = -A e^{-tA} z(t) + e^{-tA} z'(t) = -A w(t) + e^{-tA} A z(t) = 0\n\nPuisque w'(t) = 0, w(t) est constante (d’après le TAF). Donc, w(t) = w(0) = e^{-0 \\cdot A} z(0) = 0.\nAinsi, z(t) = e^{tA} w(t) = 0 pour tout t, ce qui implique que y(t) = x(t, x_0). Cela prouve l’unicité de la solution.",
    "crumbs": [
      "Plan du cours",
      "Problème de Cauchy linéaire",
      "Solution"
    ]
  },
  {
    "objectID": "src/edo_lineaires/solution.html#cas-homogène",
    "href": "src/edo_lineaires/solution.html#cas-homogène",
    "title": "Solution du Problème de Cauchy Linéaire",
    "section": "",
    "text": "Considérons le problème de Cauchy linéaire homogène suivant :\n\n    x'(t) = A\\, x(t), \\quad x(0) = x_0\n\noù A \\in M_n(\\mathbb{R}) est une matrice constante et x_0 \\in \\mathbb{R}^n est le vecteur d’état initial.\n\n\n\n\n\n\nThéorème\n\n\n\nLa solution du problème de Cauchy linéaire homogène est donnée par :\n\n    x(t, x_0) = e^{tA} x_0\n\n\n\n\n\n\n\n\n\nPreuve\n\n\n\n\n\nExistence. Montrons que x(t, x_0) = e^{tA} x_0 vérifie les propriétés d’une solution pour le problème de Cauchy.\n\nVérification de la Condition Initiale. À t = 0, nous avons : \n      x(0, x_0) = e^{0 \\cdot A} x_0 = I x_0 = x_0\n   Donc, la condition initiale est satisfaite.\nVérification de l’Équation Différentielle. Calculons la dérivée de x(t, x_0) par rapport à t : \n      \\frac{d}{dt} x(t, x_0) = \\frac{d}{dt} (e^{tA} x_0) = A e^{tA} x_0 = A x(t, x_0)\n   Donc, x(t, x_0) satisfait l’équation différentielle x'(t) = A x(t).\n\nAinsi, x(t, x_0) = e^{tA} x_0 est bien une solution du problème de Cauchy, ce qui prouve l’existence.\nUnicité. Supposons qu’il existe une autre solution y(t) du problème de Cauchy. Définissons z(t) = y(t) - x(t, x_0). Alors,\n\n    z'(t) = y'(t) - x'(t, x_0) = A y(t) - A x(t, x_0) = A (y(t) - x(t, x_0)) = A z(t)\n\nAvec la condition initiale z(0) = y(0) - x(0, x_0) = 0.\nConsidérons maintenant le changement de variable w(t) = e^{-tA} z(t). Alors,\n\n    w'(t) = \\frac{d}{dt} (e^{-tA} z(t)) = -A e^{-tA} z(t) + e^{-tA} z'(t) = -A w(t) + e^{-tA} A z(t) = 0\n\nPuisque w'(t) = 0, w(t) est constante (d’après le TAF). Donc, w(t) = w(0) = e^{-0 \\cdot A} z(0) = 0.\nAinsi, z(t) = e^{tA} w(t) = 0 pour tout t, ce qui implique que y(t) = x(t, x_0). Cela prouve l’unicité de la solution.",
    "crumbs": [
      "Plan du cours",
      "Problème de Cauchy linéaire",
      "Solution"
    ]
  },
  {
    "objectID": "src/edo_lineaires/solution.html#cas-non-homogène",
    "href": "src/edo_lineaires/solution.html#cas-non-homogène",
    "title": "Solution du Problème de Cauchy Linéaire",
    "section": "Cas Non Homogène",
    "text": "Cas Non Homogène\nConsidérons maintenant le problème de Cauchy linéaire non homogène :\n\n    x'(t) = A x(t) + b(t), \\quad x(t_0) = x_0,\n\noù A \\in M_n(\\mathbb{R}) est une matrice constante et t \\mapsto b(t) \\in \\mathbb{R}^n est définie et continue sur un intervalle ouvert \\mathcal{I} contenant t_0.\n\n\n\n\n\n\nNote\n\n\n\nNous supposons acquis l’existence d’une unique solution globale.\n\n\n\n\n\n\n\n\nThéorème\n\n\n\nLa solution du problème de Cauchy linéaire non homogène est donnée par : \nx(t) = e^{(t-t_0)A}\\, x_0 + \\int_{t_0}^{t} e^{(t-s)A}\\, b(s) \\, ds\n\n\n\n\n\n\n\n\n\nPreuve\n\n\n\n\n\nPuisque nous supposons l’existence d’une unique solution globale, il suffit de vérifier que la fonction x(t) définie dans le théorème satisfait à la fois la condition initiale et l’équation différentielle. Cette vérification est laissée au lecteur. Pour retrouver son expression en s’appuyant sur la solution du cas homogène, on utilise la méthode de variation des constantes.\nMéthode de variation des constantes. Posons x(t) = e^{(t-t_0)A} z(t) et déterminons z(t). Tout d’abord, x(\\cdot) doit vérifier la condition initiale :\n\nx(t_0) = e^{(t_0-t_0)A} z(t_0) = z(t_0) = x_0,\n\nd’où z(t_0) = x_0.\nEnsuite, x(\\cdot) doit satisfaire l’équation différentielle. En dérivant, on obtient :\n\nx'(t) = A\\, x(t) + e^{(t-t_0)A} z'(t).\n\nIl faut donc choisir z(\\cdot) de manière à ce que b(t) = e^{(t-t_0)A} z'(t), ce qui équivaut à z'(t) = e^{(t_0-t)A} b(t).\nEnfin, en intégrant :\n\nz(t) = x_0 + \\int_{t_0}^t e^{(t_0-s)A}\\, b(s)\\, \\mathrm{d}s.\n\nD’où l’expression finale :\n\nx(t) = e^{(t-t_0)A} z(t) = e^{(t-t_0)A} x_0 + \\int_{t_0}^t e^{(t-s)A}\\, b(s)\\, \\mathrm{d}s,\n\ngrâce à la propriété e^{(t-t_0)A} e^{(t_0-s)A} = e^{(t-s)A}.",
    "crumbs": [
      "Plan du cours",
      "Problème de Cauchy linéaire",
      "Solution"
    ]
  },
  {
    "objectID": "src/edo_lineaires/solution.html#cas-particulier-b-constant",
    "href": "src/edo_lineaires/solution.html#cas-particulier-b-constant",
    "title": "Solution du Problème de Cauchy Linéaire",
    "section": "Cas Particulier : b Constant",
    "text": "Cas Particulier : b Constant\nSi b est un vecteur constant (ne dépend pas du temps), nous pouvons nous ramener au cas linéaire homogène par un changement de variable. On fixe pour la suite t_0 = 0.\nPosons z(t) = x(t) - x_p(t), où x_p(t) est une solution particulière de l’équation x'(t) = A x(t) + b. En substituant x(t) = z(t) + x_p(t) dans l’équation originale, nous obtenons : \n    z'(t) + x'_p(t) = A(z(t) + x_p(t)) + b.\n\nPuisque x_p(t) est une solution particulière, x'_p(t) = A x_p(t) + b. Donc, \n    z'(t) = A z(t).\n\nAinsi, z(t) satisfait une équation différentielle linéaire homogène. La solution de z'(t) = A z(t) avec la condition initiale z(0) = x_0 - x_p(0) est donnée par : \n    z(t) = e^{tA} (x_0 - x_p(0)).\n\nEn revenant à la variable originale, nous avons : \n    x(t) = z(t) + x_p(t) = e^{tA} (x_0 - x_p(0)) + x_p(t).\n\nCe changement de variable permet de ramener le problème linéaire non homogène à un problème linéaire homogène, facilitant ainsi sa résolution.\n\n\n\n\n\n\nNote\n\n\n\nSi b appartient à l’image de A, c’est-à-dire s’il existe un vecteur x_e tel que A x_e = -b, alors nous pouvons choisir x_p(t) = x_e. En effet, dans ce cas, x_e est une solution particulière constante de l’équation x'(t) = A x(t) + b, car : \nA x_e + b = -b + b = 0\n\nAinsi, x_p(t) = x_e est une solution particulière constante, simplifiant encore le changement de variable et nous obtenons \n    x(t) = e^{tA} (x_0 - x_e) + x_e.",
    "crumbs": [
      "Plan du cours",
      "Problème de Cauchy linéaire",
      "Solution"
    ]
  },
  {
    "objectID": "src/integration_numerique/euler.html",
    "href": "src/integration_numerique/euler.html",
    "title": "Méthode d’Euler",
    "section": "",
    "text": "La méthode d’Euler est une technique simple et intuitive pour résoudre numériquement les équations différentielles. Elle est souvent utilisée pour les systèmes linéaires et permet de trouver des approximations de la solution au fil du temps.",
    "crumbs": [
      "Plan du cours",
      "Intégration numérique",
      "Méthode d'Euler"
    ]
  },
  {
    "objectID": "src/integration_numerique/euler.html#introduction-à-la-méthode-deuler",
    "href": "src/integration_numerique/euler.html#introduction-à-la-méthode-deuler",
    "title": "Méthode d’Euler",
    "section": "Introduction à la Méthode d’Euler",
    "text": "Introduction à la Méthode d’Euler\nConsidérons une équation différentielle du premier ordre :\n\n\\dot{x}(t) = f(t, x(t))\n\noù ( f(t, x(t)) ) est une fonction qui décrit l’évolution du système à un instant donné. Dans le cas d’un système linéaire, la fonction ( f ) est souvent de la forme :\n\nf(t, x(t)) = A x(t)\n\noù ( A ) est une matrice constante et ( x(t) ) est un vecteur d’état.",
    "crumbs": [
      "Plan du cours",
      "Intégration numérique",
      "Méthode d'Euler"
    ]
  },
  {
    "objectID": "src/integration_numerique/euler.html#schéma-deuler",
    "href": "src/integration_numerique/euler.html#schéma-deuler",
    "title": "Méthode d’Euler",
    "section": "Schéma d’Euler",
    "text": "Schéma d’Euler\nLa méthode d’Euler consiste à discrétiser le temps en utilisant une grille uniforme ( t_i = t_0 + ih ), où ( h ) est le pas de temps. À chaque instant ( t_i ), on obtient une approximation de la solution ( x(t_i) ), notée ( x_i ). Le schéma d’Euler est donné par :\n\nx_{i+1} = x_i + h f(t_i, x_i)\n\nDans le cas d’un système linéaire, cela devient :\n\nx_{i+1} = x_i + h A x_i = (I + h A) x_i\n\noù ( I ) est la matrice identité de taille appropriée.",
    "crumbs": [
      "Plan du cours",
      "Intégration numérique",
      "Méthode d'Euler"
    ]
  },
  {
    "objectID": "src/integration_numerique/euler.html#consistance-stabilité-et-convergence-de-la-méthode-deuler",
    "href": "src/integration_numerique/euler.html#consistance-stabilité-et-convergence-de-la-méthode-deuler",
    "title": "Méthode d’Euler",
    "section": "Consistance, Stabilité et Convergence de la Méthode d’Euler",
    "text": "Consistance, Stabilité et Convergence de la Méthode d’Euler\n\nConsistance\nLa consistance d’un schéma numérique mesure à quel point la solution numérique se rapproche de la solution exacte lorsque le pas de temps ( h ) devient petit.\nPour la méthode d’Euler appliquée à l’équation différentielle linéaire ( (t) = A x(t) ), l’erreur locale ( e_i ) à chaque étape est donnée par :\n\ne_i = x(t_{i+1}) - x(t_i) - h A x(t_i)\n\nEn utilisant le développement en série de Taylor de ( x(t) ) autour de ( t_i ), nous avons :\n\nx(t_{i+1}) = x(t_i) + h \\dot{x}(t_i) + \\frac{h^2}{2} \\ddot{x}(\\xi)\n\npour un certain ( (t_i, t_{i+1}) ). Comme ( (t_i) = A x(t_i) ), cela donne :\n\ne_i = h A x(t_i) + \\frac{h^2}{2} \\ddot{x}(\\xi) - h A x(t_i) = \\frac{h^2}{2} \\ddot{x}(\\xi)\n\nAinsi, l’erreur locale est d’ordre ( O(h^2) ), ce qui signifie que la méthode d’Euler est consistante d’ordre 1.\n\n\nStabilité\nLa stabilité d’un schéma numérique garantit que les erreurs d’approximation ne s’amplifient pas au cours du temps. Pour la méthode d’Euler appliquée à ( (t) = A x(t) ), considérons deux suites ( (x_i) ) et ( (y_i) ) définies par :\n\nx_{i+1} = x_i + h A x_i\n\n\ny_{i+1} = y_i + h A y_i + \\epsilon_i\n\noù ( _i ) est une perturbation. Le schéma est stable s’il existe une constante ( S &gt; 0 ) indépendante de ( h ) telle que :\n\n\\| x_i - y_i \\| \\leq S \\| x_0 - y_0 \\| + S \\max_{0 \\leq j \\leq i} \\| \\epsilon_j \\|\n\nCela signifie que de petites perturbations ne conduisent pas à des erreurs de plus en plus grandes, indépendamment de la taille du pas de temps.\n\n\nConvergence\nLa convergence d’un schéma numérique signifie que la solution numérique ( x_i ) se rapproche de la solution exacte ( x(t_i) ) lorsque ( h ).\nLa convergence de la méthode d’Euler repose sur les théorèmes de consistance et de stabilité (Théorème de Lax-Richtmyer). Comme nous avons montré que la méthode est consistante d’ordre 1 et stable, elle est également convergente. Cela implique que :\n\nx_i \\to x(t_i) \\quad \\text{lorsque} \\quad h \\to 0\n\nEn résumé, pour un système linéaire ( (t) = A x(t) ), la méthode d’Euler est convergente si les valeurs propres de ( A ) ont des parties réelles négatives, ce qui assure que ( x_i ) approche ( x(t_i) ) lorsque ( h ) devient petit.",
    "crumbs": [
      "Plan du cours",
      "Intégration numérique",
      "Méthode d'Euler"
    ]
  },
  {
    "objectID": "src/derivees/fonctions.html",
    "href": "src/derivees/fonctions.html",
    "title": "Fonctions Dérivées",
    "section": "",
    "text": "Considérons une équation différentielle ordinaire dépendant d’une condition initiale :\n\n\\dot{x}(t) = f(t, x(t)), \\quad x(t_0) = x_0\n\nNous souhaitons étudier comment la solution x(t) varie en fonction de la condition initiale x_0.",
    "crumbs": [
      "Plan du cours",
      "Dérivées",
      "Fonctions dérivées"
    ]
  },
  {
    "objectID": "src/derivees/fonctions.html#fonctions-dérivées-pour-une-équation-différentielle-ordinaire-edo",
    "href": "src/derivees/fonctions.html#fonctions-dérivées-pour-une-équation-différentielle-ordinaire-edo",
    "title": "Fonctions Dérivées",
    "section": "",
    "text": "Considérons une équation différentielle ordinaire dépendant d’une condition initiale :\n\n\\dot{x}(t) = f(t, x(t)), \\quad x(t_0) = x_0\n\nNous souhaitons étudier comment la solution x(t) varie en fonction de la condition initiale x_0.",
    "crumbs": [
      "Plan du cours",
      "Dérivées",
      "Fonctions dérivées"
    ]
  },
  {
    "objectID": "src/derivees/fonctions.html#dérivée-par-rapport-à-la-condition-initiale",
    "href": "src/derivees/fonctions.html#dérivée-par-rapport-à-la-condition-initiale",
    "title": "Fonctions Dérivées",
    "section": "Dérivée par Rapport à la Condition Initiale",
    "text": "Dérivée par Rapport à la Condition Initiale\nSoit X(t) = \\frac{\\partial x(t)}{\\partial x_0}, la dérivée de la solution x(t) par rapport à la condition initiale x_0. Nous cherchons à déterminer l’équation différentielle satisfaite par X(t).\n\n1. Équation Intégrale\nLa solution de l’EDO peut être écrite sous la forme intégrale suivante :\n\nx(t) = x_0 + \\int_{t_0}^{t} f(s, x(s)) \\, ds\n\n\n\n2. Dérivation par Rapport à x_0\nEn dérivant cette équation par rapport à x_0, nous obtenons l’équation suivante pour X(t) :\n\nX'(t) = \\frac{\\partial f}{\\partial x}(t, x(t)) \\, X(t)\n\nAvec la condition initiale :\n\nX(t_0) = I\n\nOù I est la matrice identité (si x(t) est un vecteur, ou simplement 1 si x(t) est un scalaire), et \\frac{\\partial f}{\\partial x} est la jacobienne de f par rapport à x.\n\n\n3. Équation Variationnelle\nLa fonction X(t) satisfait ainsi une équation différentielle linéaire appelée équation variationnelle :\n\nX'(t) = \\frac{\\partial f}{\\partial x}(t, x(t)) \\, X(t)\n\nAvec la condition initiale :\n\nX(t_0) = I\n\nCette équation décrit comment les petites variations de la condition initiale x_0 influencent l’évolution de la solution x(t) dans le temps.",
    "crumbs": [
      "Plan du cours",
      "Dérivées",
      "Fonctions dérivées"
    ]
  },
  {
    "objectID": "src/derivees/fonctions.html#introduction-de-paramètres",
    "href": "src/derivees/fonctions.html#introduction-de-paramètres",
    "title": "Fonctions Dérivées",
    "section": "Introduction de Paramètres",
    "text": "Introduction de Paramètres\nConsidérons maintenant une EDO dépendant de paramètres \\theta :\n\n\\dot{x}(t) = f(t, x(t), \\theta), \\quad x(t_0) = x_0\n\nNous souhaitons trouver la dérivée de la solution x(t) par rapport aux paramètres \\theta.\n\n4. Équation Intégrale avec Paramètres\nLa solution de cette EDO avec les paramètres \\theta peut être écrite sous la forme intégrale :\n\nx(t, \\theta) = x_0 + \\int_{t_0}^{t} f(s, x(s, \\theta), \\theta) \\, ds\n\n\n\n5. Dérivation par Rapport à \\theta\nLa dérivée de x(t) par rapport à \\theta donne une équation variationnelle pour \\frac{\\partial x}{\\partial \\theta}(t, \\theta), et peut être obtenue en différentiant l’équation intégrale par rapport à \\theta :\n\n\\frac{\\partial x}{\\partial \\theta}(t, \\theta) = \\int_{t_0}^{t} \\frac{\\partial f}{\\partial \\theta}(s, x(s, \\theta), \\theta) \\, ds\n\nCette dérivation permet d’obtenir une sensibilité de la solution x(t) par rapport aux paramètres \\theta.",
    "crumbs": [
      "Plan du cours",
      "Dérivées",
      "Fonctions dérivées"
    ]
  },
  {
    "objectID": "src/derivees/fonctions.html#combinaison-des-dérivées",
    "href": "src/derivees/fonctions.html#combinaison-des-dérivées",
    "title": "Fonctions Dérivées",
    "section": "Combinaison des Dérivées",
    "text": "Combinaison des Dérivées\nLorsque la solution x(t, x_0, \\theta) dépend à la fois de la condition initiale x_0 et des paramètres \\theta, la dérivée totale de x(t, x_0, \\theta) par rapport à t est donnée par :\n\n\\frac{d}{dt} x(t, x_0, \\theta) = \\frac{\\partial x}{\\partial t}(t, x_0, \\theta) + \\frac{\\partial x}{\\partial x_0}(t, x_0, \\theta) \\dot{x_0} + \\frac{\\partial x}{\\partial \\theta}(t, x_0, \\theta) \\dot{\\theta}\n\n\n\\frac{\\partial x}{\\partial t}(t, x_0, \\theta) est la dérivée partielle de x par rapport à t.\n\\frac{\\partial x}{\\partial x_0}(t, x_0, \\theta) est la dérivée partielle de x par rapport à x_0.\n\\frac{\\partial x}{\\partial \\theta}(t, x_0, \\theta) est la dérivée partielle de x par rapport à \\theta.\n\nCette relation montre comment les variations de la condition initiale x_0 et des paramètres \\theta affectent la dynamique du système.",
    "crumbs": [
      "Plan du cours",
      "Dérivées",
      "Fonctions dérivées"
    ]
  }
]