---
title: "Convergence"
page-layout: article
---

{{< include ../../assets/latex/macros.qmd >}}

Étant donnée une subdivision, on souhaite avoir $x_i \approx x(t_i, t_0, x_0)$ pour tout $i \in \llbracket 0\,,\, N \rrbracket$ où ${(x_i)}_{1 \le i \le N}$ est la solution de la méthode (discrétisée) à un pas explicite et où $x(\cdot, t_0, x_0)$ est la solution du problème de Cauchy (continu).

Enfin, on souhaite que $x_i$ tende vers $x(t_i, t_0, x_0)$ quand le pas de discrétisation tend vers $0$. On introduit la définition suivante.

::: {.callout-warning icon=false}
### Définition

Une méthode à un pas explicite est **convergente** si pour toute solution $x(\cdot, t_0, x_0)$, la suite ${(x_i)}_i$ définie par $x_{i+1} = x_i + h_i\, \Phi(t_i, x_i, h_i)$ vérifie

$$
    \max_{1 \le i \le N}\, \norm{x(t_i, t_0, x_0) - x_i} \to 0
    \quad\text{quand}\quad h_{\max} = \max_i h_i \to 0.
$$
:::

::: {.callout-note icon=false}
### Remarque

La quantité $\max_{1 \le i \le N}\, \norm{x(t_i, t_0, x_0) - x_i}$ dans la définition précédente s'appelle **l'erreur globale de convergence**.
:::

Notons $x(\cdot) = x(\cdot, t_0, x_0)$ la solution du problème de Cauchy, $t_0$ et $x_0$ étant fixés. Par définition de l'erreur de consistance on a

$$
    x(t_{i+1}) = x(t_i) + h_i\, \Phi(t_i, x(t_i), h_i) + e_i.
$$

Si la méthode est stable, nous en déduisons que l'erreur globale de convergence vérifie

$$
    \max_{1 \le i \le N}\, \norm{x(t_i, t_0, x_0) - x_i} \le S \sum_{i=0}^{N-1} \norm{e_i}.
$$

Nous en déduisons.

::: {.callout-important icon=false}
### Théorème

Si la méthode à un pas explicite est stable et consistante, alors elle est convergente.
:::

::: {.callout-important icon=false}
### Corollaire

La méthode d'Euler est convergente si la fonction $f$ est de classe $\mathscr{C}^{1}$ et globalement lipschitzienne par rapport à la variable $x$.
:::

Si de plus la méthode est consistante d'ordre $p$ et si l'on peut majorer les constantes $C_i$ par une constante $C$ indépendante de ${(h_i)}_i$, on obtient
$$
    \begin{aligned}
        \max_{1 \le i \le N}\, \norm{x(t_i, t_0, x_0) - x_i}
        &\le S \sum_{i=0}^{N-1} \norm{e_i} \le S \sum_{i=0}^{N-1} C_i\, {h_i}^{p+1} \\
        &\le S\, C\, h_{\max}^p \sum_{i=0}^{N-1} h_i \le S\, C\, h_{\max}^p (t_f - t_0)
        \le M h_{\max}^p
    \end{aligned}
$$
pour une certaine constante $M$ positive indépendante de ${(h_i)}_i$.
Ainsi, l'ordre de convergence est donné par l'ordre de consistance.
