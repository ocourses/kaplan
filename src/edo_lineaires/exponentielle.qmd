---
title: "Exponentielle de Matrice"
page-layout: article
---

L'exponentielle d'une matrice est un outil fondamental pour résoudre les équations différentielles linéaires. Elle généralise la notion d'exponentielle des nombres réels aux matrices.

## Définition

On définit l'exponentielle d'une matrice $A \in \mathcal{M}_n(\mathbb{R})$ par :

$$
e^A \coloneqq \sum_{k=0}^{\infty} \frac{A^k}{k!} \in \mathcal{M}_n(\mathbb{R}).
$$

::: {.callout-note}
L'exponentielle de $A$ s'écrit aussi $\exp(A)$.
:::

::: {.callout-caution collapse="true" icon=false}
### Preuve de la convergence de la série

Pour montrer que cette série converge absolument pour toute matrice $A \in \mathcal{M}_n(\mathbb{R})$, nous allons utiliser une norme sous-multiplicative. Soit $\|\cdot\|$ une norme sous-multiplicative sur $\mathcal{M}_n(\mathbb{R})$, par exemple la [norme de Frobenius](https://fr.wikipedia.org/wiki/Norme_matricielle#Norme_de_Frobenius) ou n'importe quelle norme subordonnée (ou norme d'algèbre). Une norme sous-multiplicative satisfait pour toutes matrices $\ A, B \in \mathcal{M}_n(\mathbb{R})$ la propriété suivante :

$$
    \|A B\| \leq \|A\| \|B\|.
$$

Dans ce cas, nous pouvons majorer la norme de chaque terme de la série :

$$
    \left\| \frac{A^k}{k!} \right\| = \frac{\|A^k\|}{k!} \leq \frac{\|A\|^k}{k!}.
$$

La série $\sum_{k=0}^{\infty} \frac{\|A\|^k}{k!}$ est simplement l'exponentielle scalaire de la norme $\|A\|$ :

$$
    \sum_{k=0}^{\infty} \frac{\|A\|^k}{k!} = e^{\|A\|}.
$$

Puisque la série de droite converge, il en découle que la série de matrices $\sum_{k=0}^{\infty} \frac{A^k}{k!}$ converge absolument. Enfin, une série absolument convergente dans un espace vectoriel normé de dimension finie est convergente.
:::

## Propriétés

### Matrice Nulle

On note $0_n$ la matrice nulle de $\mathcal{M}_n(\mathbb{R})$ et $I_n$ la matrice identité. On a alors :
$$
    e^{0_n} = I_n.
$$

### Matrice Diagonale

Si $\Sigma$ est une matrice diagonale avec des éléments diagonaux $\lambda_1, \lambda_2, \dots, \lambda_n$, c'est-à-dire :

$$
    \Sigma = \text{diag}(\lambda_1, \lambda_2, \dots, \lambda_n) = 
    \begin{pmatrix}
        \lambda_1 & 0 & \cdots & 0 \\
        0 & \lambda_2 & \cdots & 0 \\
        \vdots & \vdots & \ddots & \vdots \\
        0 & 0 & \cdots & \lambda_n
    \end{pmatrix},
$$

alors l'exponentielle de $\Sigma$ est simplement la matrice diagonale des exponentielles des éléments diagonaux :

$$
    e^\Sigma = \text{diag}(e^{\lambda_1}, e^{\lambda_2}, \dots, e^{\lambda_n}) =
    \begin{pmatrix}
        e^{\lambda_1} & 0 & \cdots & 0 \\
        0 & e^{\lambda_2} & \cdots & 0 \\
        \vdots & \vdots & \ddots & \vdots \\
        0 & 0 & \cdots & e^{\lambda_n}
    \end{pmatrix}.
$$

### Matrice Semblable

Soit $A \in \mathcal{M}_n(\mathbb{R})$ et $P \in \mathrm{GL}_n(\mathbb{R})$, c'est-à-dire $P$ est inversible. Alors

$$
    e^{PAP^{-1}} = P e^A P^{-1}.
$$

::: {.callout-note}
Si la matrice $A$ est diagonalisable, on peut facilement calculer son exponentielle en se ramenant au calcul de l'exponentielle d'une matrice diagonale.
:::

::: {.callout-caution collapse="true" icon=false}
### Preuve

Nous avons :
$$
    e^{PAP^{-1}} = \sum_{k=0}^{\infty} \frac{\left(PAP^{-1}\right)^k}{k!}.
$$

Montrons par récurrence que $\left(PAP^{-1}\right)^k = P A^k P^{-1}$. 

- Pour $k=0$, nous avons $I_n = \left(PAP^{-1}\right)^0$ et $P A^0 P^{-1} = P P^{-1} = I_n$ donc la propriété est vraie. 
- Supposons cette propriété vraie pour $k > 0$. Alors
$$
    \left(PAP^{-1}\right)^{k+1} = \left(PAP^{-1}\right)^k PAP^{-1} = P A^k P^{-1} PAP^{-1} = P A^{k+1} P^{-1}
$$
et donc la propriété est vraie pour $k+1$.

Ainsi, on a :
\begin{align}
    e^{PAP^{-1}} &= \sum_{k=0}^{\infty} \frac{\left(PAP^{-1}\right)^k}{k!} = 
    \sum_{k=0}^{\infty} \left( P \frac{A^k}{k!} P^{-1} \right) \notag \\[1.5em]
    &= P \left( \sum_{k=0}^{\infty} \frac{A^k}{k!} \right) P^{-1} 
    \tag*{(par continuité de la conjugaison)} \\[1.5em]
    &= P e^A P^{-1}. \notag
\end{align}
:::

### Matrice Transposée

La transposée d'une matrice $A$ de taille $m \times n$ est notée $A^T$ et s'obtient en échangeant les lignes et les colonnes de $A$. Autrement dit, l'élément $(i, j)$ de $A^T$ est l'élément $(j, i)$ de $A$ :

$$
(A^T)_{ij} = A_{ji}
$$

Concernant l'exponentielle de matrice, nous avons :

$$
e^{A^T} = (e^A)^T.
$$

::: {.callout-note}
La preuve est laissée en exercice. On pourra s'inspirer de la preuve ci-avant.
:::

### Matrice Nilpotente

Une matrice carrée $A$ est dite nilpotente s'il existe un entier $k$ tel que :

$$
A^k = 0.
$$

Dans ce cas, l'exponentielle de $A$ s'écrit sous forme de somme finie :

$$
e^A = \sum_{n=0}^{k-1} \frac{A^n}{n!}.
$$

Par exemple, pour la matrice 

$$
A = \begin{pmatrix} 0 & 1 \\ 0 & 0 \end{pmatrix},
$$

on a $A^2 = 0$, donc son exponentielle est :

$$
e^A = I + A = \begin{pmatrix} 1 & 1 \\ 0 & 1 \end{pmatrix}.
$$

### Matrices Commutantes

Si les matrices $A$ et $B$ commutent, c'est-à-dire si $AB = BA$, alors l'exponentielle de leur somme est le produit des exponentielles :

$$
    e^{A + B} = e^A\, e^B.
$$

::: {.callout-caution collapse="true" icon=false}
### Preuve

Soit $A$ et $B$ deux matrices dans $\mathcal{M}_n(\mathbb{R})$ qui commutent, c'est-à-dire telle que $AB = BA$.

En développant $(A+B)^k$ à l'aide du binôme de Newton, on obtient puisque $A$ et $B$ commutent :
$$
    (A + B)^k = \sum_{j=0}^{k} \binom{k}{j} A^j B^{k-j}.
$$

En utilisant le produit de Cauchy et le résultat précédent, nous avons :
$$
    e^A\, e^B = \sum_{k=0}^{\infty} c_k, \quad c_k = \sum_{j=0}^{k} \frac{A^j}{j!} \frac{B^{k-j}}{(k-j)!} = \frac{(A + B)^k}{k!}.
$$

Ainsi, au final nous obtenons :
$$
    e^A\, e^B = \sum_{k=0}^{\infty} \frac{(A + B)^k}{k!} = e^{A+B}.
$$

Cela conclut la démonstration.
:::

::: {.callout-important}
La commutativité des matrices $A$ et $B$ est une condition nécessaire pour que cette propriété soit vraie. Voir la [formule de Baker-Campbell-Hausdorff](https://fr.wikipedia.org/wiki/Formule_de_Baker-Campbell-Hausdorff) pour plus de détails.

**Contre-exemple.** Soient
$$
    A = \begin{pmatrix} 0 & 1 \\ 0 & 0 \end{pmatrix}, \quad B = \begin{pmatrix} 0 & 0 \\ 1 & 0 \end{pmatrix}.
$$

Alors, pusique $A^2 = B^2 = 0_2$, on a :
$$
    e^A = \begin{pmatrix} 1 & 1 \\ 1 & 0 \end{pmatrix}, \quad e^B = \begin{pmatrix} 1 & 0 \\ 1 & 1 \end{pmatrix}
$$
mais par ailleursn $(A + B)^2 = I_2$ donc
$$
    \begin{aligned}
        e^{A+B} &= \left( \sum_{k=0}^{\infty} \frac{1}{2k!} \right) I_2 + \left( \sum_{k=0}^{\infty} \frac{1}{(2k+1)!} \right) (A+B) = 
        \cosh(1) I_2 + \sinh(1) (A + B) \\
        &\ne e^A e^B = \begin{pmatrix} 2 & 1 \\ 1 & 1 \end{pmatrix}.
    \end{aligned}
$$
:::

### Inversibilité et Dérivabilité

Soit $A \in \mathcal{M}_n(\mathbb{R})$. En corollaire de ce qui précède, nous avons les propriétés suivantes :

1. **Inversibilité** : $e^A$ est toujours une matrice inversible et son inverse est donné par :

    $$
        (e^A)^{-1} = e^{-A}.
    $$

2. **Dérivabilité** : L'application $t \mapsto e^{tA}$ est dérivable par rapport à $t$, et sa dérivée est donnée par :

    $$
        \frac{\mathrm{d}}{\mathrm{d} t} e^{tA} = A\, e^{tA} = e^{tA} A.
    $$

::: {.callout-caution collapse="true" icon=false}
### Preuve

1. **Inversibilité** :

    Pour prouver que $e^A$ est inversible et que son inverse est $e^{-A}$, nous utilisons la commutativité des matrices $A$ et $-A$. En effet, il est évident que les matrices $A$ et $-A$ commutent, donc on peut écrire :

    $$
        e^{-A} e^A = e^A e^{-A} = e^{A + (-A)} = e^{0_n} = I_n.
    $$

    Ainsi, $e^A$ est inversible et son inverse est $e^{-A}$.

2. **Dérivabilité** :

    Fixons un $t \in \mathbb{R}$. Pour étudier la dérivée de $e^{tA}$ par rapport à $t$, nous commençons par utiliser la définition de la dérivée :
    $$
        \frac{\mathrm{d}}{\mathrm{d}t} e^{tA} = \lim_{h \to 0} \frac{e^{(t+h)A} - e^{tA}}{h}.
    $$

    En remarquant que $tA$ et $hA$ commutent, nous réécrivons la différence comme suit :
    $$
        e^{(t+h)A} - e^{tA} = e^{tA} e^{hA} - e^{tA} = e^{tA} (e^{hA} - I_n).
    $$

    Divisons par $h$ :
    $$
        \frac{e^{(t+h)A} - e^{tA}}{h} = e^{tA} \frac{e^{hA} - I}{h} = e^{tA} \left( \sum_{k=1}^{\infty} \frac{h^{k-1}A^k}{k!} \right).
    $$

    Or,
    $$
        \begin{aligned}
            \sum_{k=1}^{\infty} \frac{h^{k-1}A^k}{k!} &= A + h \sum_{k=2}^{\infty} \frac{h^{k-2}A^k}{k!} \\
            &= A + h A^2 \sum_{k=0}^{\infty} \frac{h^{k}A^k}{(k+2)!}
        \end{aligned}
    $$
    et la série de termes ${h^{k}A^k}/{(k+2)!}$ est convergente puisque ceux-ci sont majorés pas ceux de l'exponentielle de $hA$. On note $B$ la limite de la série et on obtient 
    $$
        \lim_{h \to 0} \frac{e^{(t+h)A} - e^{tA}}{h} = \lim_{h \to 0} e^{tA} \left( A + h A^2 B \right) = e^{tA} A.
    $$

    Donc, la dérivée de $e^{tA}$ est :

    $$
        \frac{\mathrm{d}}{\mathrm{d}t} e^{tA} = e^{tA} A.
    $$

    Par commutativité de $A$ avec ses puissances, on peut aussi écrire cette dérivée comme :

    $$
        \frac{\mathrm{d}}{\mathrm{d}t} e^{tA} = A\, e^{tA}.
    $$

Cela conclut la démonstration des deux propriétés.
:::

::: {.callout-note}
On pourra préférer la notation $\exp$. Ces deux derniers résultats nous indique que l'on a $\exp \colon \mathcal{M}_n(\mathbb{R}) \to \mathrm{GL}_n(\mathbb{R})$ et que 
$$
    \frac{\mathrm{d}}{\mathrm{d}t} \exp(tA) = A \exp(tA) = \exp(tA) A.
$$
**Attention**, nous n'avons rien prouvé quant à la régularité de l'application $\exp$, rien au sujet ni de sa continuité sur $\mathcal{M}_n(\mathbb{R})$, ni de sa dérivabilité. Cependant, on peut montrer que cette application est continue, même $\mathscr{C}^\infty$. L'application exponentielle est même surjective.
:::